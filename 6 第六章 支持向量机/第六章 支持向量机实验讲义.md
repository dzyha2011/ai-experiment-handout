# 支持向量机实验讲义

## 一、支持向量机基本概念

### 1.1 算法原理

支持向量机（Support Vector Machine, SVM）是一种**监督学习算法**，主要用于分类任务，其核心思想是在特征空间中找到**最优超平面**，实现不同类别样本的分离。最优超平面需满足两个条件：能够正确划分样本类别；**最大化分类间隔**（Margin）——即两类样本中距离超平面最近的点（支持向量）到超平面的距离之和。

#### 1.1.1 数学形式

对于给定训练样本集 $(x_i, y_i)$（其中 $y_i \in \{+1, -1\}$ 为类别标签），超平面方程定义为：
$w^T x + b = 0$
其中 $w$ 为法向量，$b$ 为偏置项。样本点到超平面的距离为：
$\text{distance} = \frac{|w^T x + b|}{\|w\|}$
SVM的目标是最大化间隔 $2/\|w\|$，等价于最小化 $\|w\|^2/2$，同时满足约束条件：
$y_i (w^T x_i + b) \geq 1 \quad (\text{硬间隔SVM，适用于线性可分数据})$

#### 1.1.2 软间隔与惩罚参数 \(C\)

现实数据往往存在噪声或非线性可分情况，因此引入松弛变量 $\xi_i \geq 0$ 允许部分样本分类错误，目标函数调整为：
$\min_{w,b,\xi} \frac{1}{2}|w|^2 + C \sum_{i=1}^n \xi_i$

$s.t.\   y_i(w\cdot x_i +b)\geq 1-\xi_i,i=1,...,n$

$\xi\geq0,i=1,...,n$

其中$C$为惩罚系数：

- **$C$越大**：对误分类样本惩罚越重，模型易过拟合；
- **$C$越小**：允许更多误分类，模型泛化能力增强。

#### 1.1.3 核函数与非线性SVM

当数据非线性可分时，SVM通过**核函数**（Kernel Function）将低维特征空间映射到高维空间，从而实现线性可分。常见核函数包括：

| 核函数类型 | 表达式 | 适用场景 |
|------------|--------|----------|
| 线性核（Linear） | $K(x_i, x_j) = x_i^T x_j$ | 线性可分数据 |
| 多项式核（Poly） | $K(x_i, x_j) = (x_i^T x_j + c)^d$ | 特征间存在多项式关系 |
| 径向基核（RBF） | $K(x_i, x_j) = \exp(-\gamma \|x_i - x_j\|^2)$ | 非线性关系，默认选择 |
| Sigmoid核 | $K(x_i, x_j) = \tanh(\gamma x_i^T x_j + c)$ | 类神经网络场景 |

**RBF核参数 \(\gamma\)**：控制样本影响范围，\(\gamma\) 越大，支持向量影响越小，模型越复杂。

### 1.2 支持向量机的优势

- **小样本学习**：仅依赖支持向量，泛化能力强；
- **高维空间适应性**：通过核函数避免维度灾难；
- **鲁棒性**：对噪声不敏感（通过软间隔机制）。

## 二、线性支持向量机Python实现

### 2.1 实验目的

- 掌握线性SVM的建模流程；
- 理解决策边界与支持向量的关系；
- 可视化分类结果。

### 2.2 数据集选择

使用scikit-learn内置的**乳腺癌数据集**（`breast_cancer`），包含569个样本、30个特征，二分类任务（良性/恶性肿瘤）。

### 2.3 代码实现

#### 2.3.1 环境准备

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score
from sklearn.decomposition import PCA  # 用于降维可视化
```

#### 2.3.2 数据加载与预处理

```python
# 加载数据集
data = load_breast_cancer()
X, y = data.data, data.target
feature_names = data.feature_names

# 划分训练集与测试集（7:3）
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# 数据标准化（SVM对特征尺度敏感）
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

#### 2.3.3 模型训练与预测（含代码填空）

```python
# 创建线性SVM模型（填空1：设置惩罚参数C，建议尝试1.0或10.0）
svm_linear = LinearSVC(C=___, max_iter=10000, random_state=42)

# 训练模型
svm_linear.fit(X_train_scaled, y_train)

# 预测测试集
y_pred = svm_linear.predict(X_test_scaled)

# 计算准确率
accuracy = accuracy_score(y_test, y_pred)
print(f"测试集准确率: {accuracy:.4f}")
```

#### 2.3.4 结果可视化

通过PCA将特征降维至2D，绘制决策边界：

```python
# 降维至2D
pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train_scaled)

# 绘制决策边界
h = 0.02  # 网格步长
x_min, x_max = X_train_pca[:, 0].min() - 1, X_train_pca[:, 0].max() + 1
y_min, y_max = X_train_pca[:, 1].min() - 1, X_train_pca[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

# 用降维后的数据训练模型（仅用于可视化）
svm_vis = LinearSVC(C=1.0, max_iter=10000)
svm_vis.fit(X_train_pca, y_train)
Z = svm_vis.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# 绘制等高线与样本点
plt.figure(figsize=(10, 6))
plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)
plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, cmap=plt.cm.coolwarm, edgecolors='k')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.title('Linear SVM Decision Boundary (PCA-reduced Data)')
plt.show()
```

### 2.4 实验结果分析

- **支持向量**：通过 `svm_linear.support_vectors_` 可查看支持向量索引；
- **决策边界**：线性SVM在2D空间中表现为直线，高维空间中为超平面；
- **参数影响**：若 \(C\) 过大，模型可能过拟合训练集，测试集准确率下降。

## 三、非线性支持向量机Python实现

### 3.1 实验目的

- 掌握核函数（尤其是RBF）的应用；
- 对比线性与非线性SVM的分类效果；
- 理解核函数参数对模型的影响。

### 3.2 数据集选择

使用scikit-learn生成的**非线性可分数据集**（`make_moons`），模拟环形分布样本。

### 3.3 代码实现

#### 3.3.1 数据生成与可视化

```python
from sklearn.datasets import make_moons
import matplotlib.pyplot as plt

# 生成非线性数据（含噪声）
X, y = make_moons(n_samples=200, noise=0.1, random_state=42)

# 可视化数据
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Nonlinear Dataset (moons)')
plt.show()
```

#### 3.3.2 模型训练（RBF核SVM，含代码填空）

```python
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split

# 划分训练集与测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 创建RBF核SVM模型（填空2：设置gamma参数，建议尝试'scale'或0.1）
svm_rbf = SVC(kernel='rbf', gamma=___, C=1.0, random_state=42)

# 训练与预测
svm_rbf.fit(X_train, y_train)
y_pred_rbf = svm_rbf.predict(X_test)
accuracy_rbf = accuracy_score(y_test, y_pred_rbf)
print(f"RBF SVM测试集准确率: {accuracy_rbf:.4f}")
```

#### 3.3.3 不同核函数效果对比

```python
# 定义核函数列表
kernels = ['linear', 'rbf', 'poly']
titles = ['Linear Kernel', 'RBF Kernel', 'Polynomial Kernel']

# 绘制决策边界
plt.figure(figsize=(15, 5))
for i, kernel in enumerate(kernels):
    # 训练模型
    svm = SVC(kernel=kernel, gamma='scale', C=1.0, random_state=42)
    svm.fit(X_train, y_train)
    
    # 绘制决策边界
    plt.subplot(1, 3, i+1)
    h = 0.02
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')
    plt.title(titles[i])
plt.show()
```

### 3.4 实验结果分析

- **线性核**：无法处理非线性数据，分类边界为直线；
- **RBF核**：通过高斯函数将数据映射到高维，可拟合复杂边界；
- **gamma参数**：\(\gamma\) 过大易过拟合（边界复杂），过小易欠拟合（边界平滑）。

## 四、支持向量机参数调优

### 4.1 调优目标

通过优化超参数提升模型性能，核心参数包括：
- **\(C\)**：惩罚系数，控制过拟合与欠拟合平衡；
- **$\gamma$**：RBF核函数的带宽参数，控制样本影响范围；
- **核函数类型**：线性/非线性选择。

### 4.2 网格搜索（GridSearchCV）实现

#### 4.2.1 调优代码（含代码填空）

```python
from sklearn.model_selection import GridSearchCV

# 定义参数网格（填空3：补充gamma的候选值，建议添加0.01, 0.1, 1）
param_grid = {
    'C': [0.1, 1, 10],
    'gamma': [___, ___, ___],
    'kernel': ['rbf']
}

# 创建SVM模型
svm = SVC(random_state=42)

# 网格搜索（5折交叉验证）
grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train, y_train)

# 输出最佳参数与最佳交叉验证准确率
print(f"最佳参数: {grid_search.best_params_}")
print(f"最佳交叉验证准确率: {grid_search.best_score_:.4f}")

# 使用最佳模型预测测试集
best_svm = grid_search.best_estimator_
y_pred_best = best_svm.predict(X_test)
print(f"调优后测试集准确率: {accuracy_score(y_test, y_pred_best):.4f}")
```

#### 4.2.2 参数影响可视化

绘制不同 \(C\) 和 \(\gamma\) 组合的交叉验证准确率热力图：

```python
import seaborn as sns
import pandas as pd

# 提取交叉验证结果
results = pd.DataFrame(grid_search.cv_results_)
pivot = results.pivot(index='param_gamma', columns='param_C', values='mean_test_score')

# 绘制热力图
plt.figure(figsize=(8, 6))
sns.heatmap(pivot, annot=True, cmap='viridis', fmt='.4f')
plt.title('Grid Search Accuracy Heatmap (RBF Kernel)')
plt.xlabel('C')
plt.ylabel('gamma')
plt.show()
```

### 4.3 调优结果分析

- **最佳参数组合**：通常位于热力图的高准确率区域（如 $C=10, \gamma=0.1$）；
- **参数相关性**：$C$ 与 $\gamma$ 存在交互作用，大 $C$ 需配合小 $\gamma$ 避免过拟合。

## 五、实例拓展-垃圾邮件分类优化

### 5.1 任务描述

使用SVM对邮件文本进行二分类（垃圾邮件/正常邮件），数据集采用scikit-learn的 `fetch_20newsgroups`（选取与邮件相关的类别）。

### 5.2 数据预处理与特征提取

#### 5.2.1 数据加载与预处理

```python
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer

# 选择两类新闻组作为垃圾邮件/正常邮件的模拟数据
categories = ['rec.sport.hockey', 'talk.politics.misc']  # 可替换为其他类别
newsgroups = fetch_20newsgroups(
    subset='all', categories=categories, remove=('headers', 'footers', 'quotes'), random_state=42
)
X_text = newsgroups.data  # 文本数据
y = newsgroups.target    # 标签（0或1）

# 划分训练集与测试集
X_train_text, X_test_text, y_train, y_test = train_test_split(X_text, y, test_size=0.3, random_state=42)
```

#### 5.2.2 TF-IDF特征向量化

```python
# 将文本转换为TF-IDF特征
tfidf = TfidfVectorizer(max_features=5000, stop_words='english')
X_train_tfidf = tfidf.fit_transform(X_train_text)
X_test_tfidf = tfidf.transform(X_test_text)

print(f"TF-IDF特征维度: {X_train_tfidf.shape}")
```

### 5.3 模型训练与优化

```python
# 使用调优后的SVM模型
svm_spam = SVC(C=10, gamma=0.1, kernel='rbf', random_state=42)
svm_spam.fit(X_train_tfidf, y_train)

# 评估性能
y_pred_spam = svm_spam.predict(X_test_tfidf)
accuracy_spam = accuracy_score(y_test, y_pred_spam)
print(f"垃圾邮件分类准确率: {accuracy_spam:.4f}")

# 混淆矩阵
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred_spam)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=newsgroups.target_names, yticklabels=newsgroups.target_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()
```

### 5.4 结果分析

- **特征重要性**：通过 `svm_spam.coef_` 可查看关键词权重（如垃圾邮件中的"free"、"offer"等）；
- **误分类原因**：文本预处理不彻底（如未去除特殊符号）或特征维度不足。

### 5.5 思考题

- 解释SVM中“支持向量”的含义，为何它们对模型至关重要？
- 对比硬间隔SVM与软间隔SVM的适用场景，举例说明何时需要软间隔。

- 在垃圾邮件分类中，若模型过拟合，应如何调整参数 \(C\) 和 \(\gamma\)？
- 尝试用多项式核（poly）替换RBF核，对比分类效果并分析原因。

- SVM与逻辑回归的异同点是什么？在高维稀疏数据（如文本）上哪个更有优势？

## 六、代码填空答案

1. **填空1**：`1.0`（或尝试`10.0`，观察准确率变化）  
2. **填空2**：`'scale'`（或`0.1`，`scale`表示 $\gamma = 1/(n\_features \cdot X.var())$）  
3. **填空3**：`0.01, 0.1, 1`（完整参数网格：`{'C': [0.1, 1, 10], 'gamma': [0.01, 0.1, 1], 'kernel': ['rbf']}`）  
4. **填空4**：`5000`（或`10000`，控制特征维度避免过拟合）  
