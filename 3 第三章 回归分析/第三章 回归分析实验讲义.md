# 人工智能课程实验讲义：回归分析

## 1. 回归分析基础理论

### 1.1 回归分析的定义与类型
回归分析是一种统计建模方法，用于研究变量之间的因果关系，通过建立数学模型来预测因变量的值。根据自变量数量和函数关系形式，可分为：
- **简单线性回归**：单个自变量与因变量的线性关系（$y = \beta_0 + \beta_1x + \epsilon$）
- **多元线性回归**：多个自变量与因变量的线性关系（$y = \beta_0 + \beta_1x_1 + ... + \beta_nx_n + \epsilon$）
- **非线性回归**：变量间存在非线性关系（如多项式回归、逻辑回归）

### 1.2 线性回归的数学原理
#### 1.2.1 最小二乘法（OLS）
线性回归的核心是通过最小化**残差平方和（RSS）** 估计参数：
$$
RSS = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 = \sum_{i=1}^{n}(y_i - (\beta_0 + \beta_1x_i))^2
$$
对 $\beta_0$ 和 $\beta_1$ 求偏导并令其为0，得到参数估计公式：
$$
\hat{\beta}_1 = \frac{n\sum x_iy_i - (\sum x_i)(\sum y_i)}{n\sum x_i^2 - (\sum x_i)^2}, \quad \hat{\beta}_0 = \bar{y} - \hat{\beta}_1\bar{x}
$$

#### 1.2.2 矩阵形式推导
对于多元线性回归，模型可表示为 $\mathbf{y} = \mathbf{X}\beta + \epsilon$，其中 $\mathbf{X}$ 为设计矩阵。参数估计的矩阵解为：
$$
\hat{\beta} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
$$

### 1.3 回归模型的假设条件
1. **线性性**：因变量与自变量间存在线性关系
2. **无多重共线性**：自变量间不存在高度相关性
3. **误差项独立性**：残差 $\epsilon_i$ 相互独立
4. **误差项同方差性**：$Var(\epsilon_i) = \sigma^2$ 为常数
5. **误差项正态分布**：$\epsilon_i \sim N(0, \sigma^2)$

### 1.4 统计显著性检验
- **t检验**：检验单个自变量的显著性（$H_0: \beta_j = 0$）
- **F检验**：检验整体模型的显著性（$H_0: \beta_1 = \beta_2 = ... = \beta_n = 0$）
- **p值**：判断检验结果是否显著（通常以 $\alpha = 0.05$ 为阈值）

## 2. Python 常用回归分析库介绍

### 2.1 核心库功能对比
| 库名 | 核心功能 | 典型应用场景 |
|------|----------|--------------|
| **NumPy** | 数值计算、矩阵运算 | 数据预处理、参数计算 |
| **Pandas** | 数据清洗、特征工程 | 数据集加载与处理 |
| **Matplotlib/Seaborn** | 数据可视化 | 绘制散点图、残差图 |
| **Scikit-learn** | 机器学习模型实现 | 线性回归、正则化模型 |
| **Statsmodels** | 统计建模与检验 | 回归系数显著性分析 |

### 2.2 关键库使用示例
#### 2.2.1 Scikit-learn 线性回归接口
```python
from sklearn.linear_model import LinearRegression
model = LinearRegression(fit_intercept=True)  # 是否计算截距
model.fit(X_train, y_train)  # 训练模型
y_pred = model.predict(X_test)  # 预测
print("系数:", model.coef_)
print("截距:", model.intercept_)
```

#### 2.2.2 Statsmodels 统计检验
```python
import statsmodels.api as sm
X = sm.add_constant(X)  # 添加截距项
model = sm.OLS(y, X).fit()  # 普通最小二乘模型
print(model.summary())  # 输出详细统计结果
```

## 3. 简单线性回归实验

### 3.1 实验目的
1. 掌握简单线性回归模型的构建流程
2. 理解最小二乘法的参数估计原理
3. 学会使用 Scikit-learn 实现回归分析

### 3.2 实验环境
- Python 3.8+
- 库：numpy, pandas, matplotlib, scikit-learn

### 3.3 实验数据集
使用 **Scikit-learn 糖尿病数据集**（特征为年龄、体重等，目标为疾病进展指标）：
```python
from sklearn.datasets import load_diabetes
data = load_diabetes()
X = data.data[:, 2].reshape(-1, 1)  # 取第3个特征（体重指数）
y = data.target
```

### 3.4 实验步骤与代码实现
#### 步骤1：数据可视化
```python
import matplotlib.pyplot as plt
plt.scatter(X, y, color='blue', alpha=0.5)
plt.xlabel('Body Mass Index (BMI)')
plt.ylabel('Disease Progression')
plt.title('Relationship Between BMI and Disease Progression')
plt.show()
```

#### 步骤2：数据划分与模型训练
```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# 数据划分（填空1：补充train_test_split参数）
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 模型初始化与训练（填空2：补充模型训练代码）
model = LinearRegression()
model.fit(_____, _____)  # 填入训练数据

# 预测
y_pred = model.predict(X_test)
```

#### 步骤3：模型参数与可视化
```python
# 提取参数（填空3：获取系数和截距）
beta_1 = model._____  # 系数
beta_0 = model._____  # 截距
print(f"回归方程: y = {beta_0:.2f} + {beta_1[0]:.2f}x")

# 绘制回归线
plt.scatter(X_test, y_test, color='blue', label='Test Data')
plt.plot(X_test, y_pred, color='red', linewidth=2, label='Regression Line')
plt.legend()
plt.show()
```

## 4. 多元线性回归实验

### 4.1 实验目的
1. 掌握多特征回归模型的构建方法
2. 理解特征相关性对模型的影响
3. 学会评估多元回归模型的性能

### 4.2 实验数据集
使用 **波士顿房价数据集**（已迁移至 `sklearn.datasets.fetch_openml`）：
```python
import pandas as pd
from sklearn.datasets import fetch_openml
data = fetch_openml(name='boston', version=1, as_frame=True)
X = data.data  # 13个特征（如犯罪率、平均房间数）
y = data.target  # 房价中位数
```

### 4.3 实验步骤与代码实现
#### 步骤1：特征相关性分析
```python
import seaborn as sns
corr = X.corr()
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Feature Correlation Matrix')
plt.show()
```

#### 步骤2：模型训练与评估
```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# 数据划分
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 训练多元线性回归模型（填空4：补充模型训练代码）
model = _____  # 初始化模型
model.fit(_____, _____)  # 填入训练数据

# 预测与评估（填空5：计算MSE和R²）
y_pred = model.predict(X_test)
mse = mean_squared_error(_____, _____)  # 填入真实值和预测值
r2 = r2_score(_____, _____)
print(f"MSE: {mse:.2f}, R²: {r2:.2f}")
```

#### 步骤3：特征重要性分析
```python
# 绘制系数热力图
coef = pd.Series(model.coef_, index=X.columns)
coef.sort_values().plot(kind='barh')
plt.title('Feature Coefficients')
plt.show()
```

## 5. 回归模型评估与改进

### 5.1 常用评估指标
| 指标 | 公式 | 意义 |
|------|------|------|
| **均方误差（MSE）** | $MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$ | 衡量预测值与真实值的平均平方误差 |
| **均方根误差（RMSE）** | $RMSE = \sqrt{MSE}$ | 与目标变量同量纲，更直观 |
| **平均绝对误差（MAE）** | $MAE = \frac{1}{n}\sum_{i=1}^{n}\|y_i - \hat{y}_i\|$ | 对异常值更稳健 |
| **决定系数（R²）** | $R^2 = 1 - \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \bar{y})^2}$ | 模型解释因变量变异的比例（范围：0~1） |

### 5.2 模型改进方法
#### 5.2.1 正则化（解决过拟合）
- **Ridge回归（L2正则化）**：损失函数 $L = RSS + \lambda\sum\beta_j^2$
  ```python
  from sklearn.linear_model import Ridge
  model = Ridge(alpha=1.0)  # alpha为正则化强度
  ```
- **Lasso回归（L1正则化）**：损失函数 $L = RSS + \lambda\sum|\beta_j|$（可实现特征选择）
  ```python
  from sklearn.linear_model import Lasso
  model = Lasso(alpha=0.1)
  ```

#### 5.2.2 特征工程
- **多项式特征**：通过 `PolynomialFeatures` 构建非线性关系
  ```python
  from sklearn.preprocessing import PolynomialFeatures
  poly = PolynomialFeatures(degree=2)  # 二次多项式
  X_poly = poly.fit_transform(X)
  ```
- **特征选择**：使用 `SelectKBest` 或递归特征消除（RFE）
  ```python
  from sklearn.feature_selection import SelectKBest, f_regression
  selector = SelectKBest(f_regression, k=5)  # 选择Top5特征
  X_selected = selector.fit_transform(X, y)
  ```

#### 5.2.3 交叉验证（避免过拟合）
```python
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')
print("CV RMSE:", np.sqrt(-scores.mean()))
```

## 6. 实例拓展-基于回归分析的图像恢复

### 6.1 实验背景
图像在传输或存储过程中易受噪声干扰，本实验通过**区域二元线性回归模型**预测噪声点像素值，实现图像去噪恢复。

### 6.2 实验原理
1. **噪声生成**：通过随机掩码将图像部分像素置零（噪声比率：0.4/0.6/0.8）
2. **区域回归**：对每个噪声点，利用其周围 $size \times size$ 邻域像素训练线性回归模型，预测噪声点像素值
3. **误差评估**：使用恢复图像与原始图像的**2-范数**衡量恢复效果：
   $$
   error = \| R(:) - I(:) \|_2
   $$

### 6.3 实验步骤
#### 步骤1：图像读取与预处理
```python
import cv2
import numpy as np
from matplotlib import pyplot as plt

def read_image(img_path):
    img = cv2.imread(img_path)
    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # 转换为RGB格式

def normalization(image):
    return image.astype(np.double) / np.iinfo(image.dtype).max  # 归一化到[0,1]

img = read_image("A.png")
nor_img = normalization(img)
plt.imshow(nor_img)
plt.axis('off')
plt.show()
```

#### 步骤2：生成受损图像
```python
def noise_mask_image(img, noise_ratio):
    noise_img = np.copy(img)
    h, w, c = img.shape
    for i in range(c):
        for j in range(h):
            # 生成噪声掩码（填空6：补充随机采样代码）
            mask = np.random.choice([0, 1], size=w, p=[_____, _____])  # 填入噪声概率和保留概率
            noise_img[j, mask == 0, i] = 0  # 噪声点置零
    return noise_img

noise_img = noise_mask_image(nor_img, noise_ratio=0.6)
plt.imshow(noise_img)
plt.title("Noisy Image (60% noise)")
plt.show()
```

#### 步骤3：区域线性回归恢复
```python
from sklearn.linear_model import LinearRegression

def restore_image(noise_img, size=4):
    res_img = np.copy(noise_img)
    noise_mask = (noise_img == 0).astype(np.double)  # 噪声点掩码（0表示噪声）
    h, w, c = noise_img.shape
    
    for row in range(h):
        for col in range(w):
            for chan in range(c):
                if noise_mask[row, col, chan] == 0:  # 非噪声点跳过
                    continue
                # 获取邻域区域（填空7：补充区域边界处理）
                rowl = max(0, row - size)
                rowr = min(h, row + size + 1)
                coll = max(0, col - size)
                colr = min(w, col + size + 1)
                
                # 提取邻域非噪声点作为训练数据
                neighbors = noise_img[rowl:rowr, coll:colr, chan]
                mask = noise_mask[rowl:rowr, coll:colr, chan] == 0
                X_train = np.argwhere(mask)  # 邻域像素坐标
                y_train = neighbors[mask].reshape(-1, 1)
                
                if len(X_train) == 0:
                    continue
                # 训练模型并预测（填空8：补充模型训练与预测）
                model = LinearRegression()
                model.fit(_____, _____)  # 填入训练数据
                res_img[row, col, chan] = model.predict([[row - rowl, col - coll]])  # 相对坐标
    return res_img

res_img = restore_image(noise_img)
plt.imshow(res_img)
plt.title("Restored Image")
plt.show()
```

#### 步骤4：恢复效果评估
```python
def compute_error(res_img, original_img):
    return np.linalg.norm(res_img.flatten() - original_img.flatten())

error = compute_error(res_img, nor_img)
print(f"恢复误差（2-范数）：{error:.2f}")
```

### 6.4 思考题
1. **噪声比率对恢复效果的影响**：当噪声比率从0.4增加到0.8时，恢复误差如何变化？为什么？
2. **邻域大小（size参数）的选择**：过大或过小的邻域会导致什么问题？如何通过实验确定最优size？
3. **模型改进方向**：除了线性回归，还可以使用哪些模型（如 Ridge/Lasso 或非线性模型）提升恢复效果？
4. **计算效率优化**：当前逐像素训练的方式计算量较大，如何通过向量化或并行计算优化？

## 7. 代码填空答案

### 3.4 简单线性回归填空
- 填空2：`X_train, y_train`
- 填空3：`coef_, intercept_`

### 4.3 多元线性回归填空
- 填空4：`LinearRegression(), X_train, y_train`
- 填空5：`y_test, y_pred, y_test, y_pred`

### 6.3 图像恢复填空
- 填空6：`noise_ratio, 1-noise_ratio`（噪声概率和保留概率）
- 填空8：`X_train, y_train`

## 附录：参考资料
1. Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning*. Springer.
2. Scikit-learn 官方文档: https://scikit-learn.org/stable/modules/linear_model.html
3. OpenCV 图像操作指南: https://opencv-python-tutroals.readthedocs.io/