# SVMå®éªŒä»£ç æµ‹è¯•
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

def test_linear_svm():
    print("=== çº¿æ€§SVMæµ‹è¯• ===")
    
    # åŠ è½½ä¹³è…ºç™Œæ•°æ®é›†
    data = load_breast_cancer()
    X, y = data.data, data.target
    feature_names = data.feature_names
    
    print(f"æ•°æ®é›†å½¢çŠ¶: {X.shape}")
    print(f"ç‰¹å¾æ•°é‡: {len(feature_names)}")
    print(f"æ ·æœ¬æ•°é‡: {len(X)}")
    print(f"ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y)}")
    print()
    
    # åˆ’åˆ†è®­ç»ƒé›†ä¸æµ‹è¯•é›†(7:3)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )
    
    # æ•°æ®æ ‡å‡†åŒ–(SVMå¯¹ç‰¹å¾å°ºåº¦æ•æ„Ÿ)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    print(f"è®­ç»ƒé›†å½¢çŠ¶: {X_train_scaled.shape}")
    print(f"æµ‹è¯•é›†å½¢çŠ¶: {X_test_scaled.shape}")
    print(f"ç‰¹å¾æ ‡å‡†åŒ–åçš„å‡å€¼: {X_train_scaled.mean():.4f}")
    print(f"ç‰¹å¾æ ‡å‡†åŒ–åçš„æ ‡å‡†å·®: {X_train_scaled.std():.4f}")
    print()
    
    # åˆ›å»ºçº¿æ€§SVMæ¨¡å‹
    svm_linear = SVC(kernel='linear', C=1.0, random_state=42)
    
    # è®­ç»ƒæ¨¡å‹
    svm_linear.fit(X_train_scaled, y_train)
    
    # é¢„æµ‹æµ‹è¯•é›†
    y_pred = svm_linear.predict(X_test_scaled)
    
    # è®¡ç®—å‡†ç¡®ç‡
    accuracy = accuracy_score(y_test, y_pred)
    print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.4f}")
    
    # æŸ¥çœ‹æ”¯æŒå‘é‡æ•°é‡
    print(f"æ”¯æŒå‘é‡æ•°é‡: {len(svm_linear.support_)}")
    print(f"å†³ç­–å‡½æ•°ç³»æ•°å½¢çŠ¶: {svm_linear.coef_.shape}")
    print()
    
    return {
        'accuracy': accuracy,
        'support_vectors': len(svm_linear.support_),
        'coef_shape': svm_linear.coef_.shape
    }

def test_nonlinear_svm():
    print("=== éçº¿æ€§SVMæµ‹è¯• ===")
    
    from sklearn.datasets import make_moons
    
    # ç”Ÿæˆéçº¿æ€§æ•°æ®(å«å™ªå£°)
    X, y = make_moons(n_samples=200, noise=0.1, random_state=42)
    
    print(f"æ•°æ®é›†å½¢çŠ¶: {X.shape}")
    print(f"ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y)}")
    print()
    
    # åˆ’åˆ†è®­ç»ƒé›†ä¸æµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    # å®šä¹‰æ ¸å‡½æ•°åˆ—è¡¨
    kernels = ['linear', 'rbf', 'poly']
    titles = ['Linear Kernel', 'RBF Kernel', 'Polynomial Kernel']
    accuracies = []
    
    # è®­ç»ƒä¸åŒæ ¸å‡½æ•°çš„SVM
    for kernel in kernels:
        svm = SVC(kernel=kernel, gamma='scale', C=1.0, random_state=42)
        svm.fit(X_train, y_train)
        y_pred = svm.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        accuracies.append(accuracy)
        print(f"{kernel.upper()}æ ¸å‡†ç¡®ç‡: {accuracy:.4f}")
    
    best_kernel_idx = np.argmax(accuracies)
    best_kernel = kernels[best_kernel_idx].upper()
    best_accuracy = accuracies[best_kernel_idx]
    
    print(f"\nğŸ† æœ€ä½³æ ¸å‡½æ•°: {best_kernel} (å‡†ç¡®ç‡: {best_accuracy:.4f})")
    print()
    
    return {
        'linear_accuracy': accuracies[0],
        'rbf_accuracy': accuracies[1],
        'poly_accuracy': accuracies[2],
        'best_kernel': best_kernel,
        'best_accuracy': best_accuracy
    }

def test_parameter_optimization():
    print("=== å‚æ•°ä¼˜åŒ–æµ‹è¯• ===")
    
    from sklearn.model_selection import GridSearchCV
    from sklearn.datasets import make_classification
    
    # ç”Ÿæˆåˆ†ç±»æ•°æ®é›†
    X, y = make_classification(n_samples=300, n_features=10, n_classes=2, random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # ç½‘æ ¼æœç´¢å‚æ•°
    param_grid = {
        'C': [0.1, 1, 10],
        'gamma': [0.01, 0.1, 1.0],
        'kernel': ['rbf']
    }
    
    # ç½‘æ ¼æœç´¢
    grid_search = GridSearchCV(SVC(random_state=42), param_grid, cv=3, scoring='accuracy')
    grid_search.fit(X_train_scaled, y_train)
    
    print(f"ğŸ” å¼€å§‹ç½‘æ ¼æœç´¢...")
    print(f"ğŸ† æœ€ä½³å‚æ•°: {grid_search.best_params_}")
    print(f"ğŸ“Š æœ€ä½³äº¤å‰éªŒè¯å‡†ç¡®ç‡: {grid_search.best_score_:.4f}")
    
    # æµ‹è¯•é›†è¯„ä¼°
    best_model = grid_search.best_estimator_
    test_accuracy = best_model.score(X_test_scaled, y_test)
    print(f"ğŸ¯ è°ƒä¼˜åæµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.4f}")
    print()
    
    return {
        'best_params': grid_search.best_params_,
        'best_cv_score': grid_search.best_score_,
        'test_accuracy': test_accuracy
    }

def test_spam_classification():
    print("=== åƒåœ¾é‚®ä»¶åˆ†ç±»æµ‹è¯• ===")
    
    from sklearn.datasets import fetch_20newsgroups
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics import classification_report, confusion_matrix
    
    # åŠ è½½æ–°é—»ç»„æ•°æ®é›†çš„å­é›†
    categories = ['rec.sport.hockey', 'talk.politics.misc']
    
    # åŠ è½½è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
    newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, random_state=42)
    newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, random_state=42)
    
    print(f"ğŸ“§ é‚®ä»¶æ•°æ®é›†ä¿¡æ¯:")
    print(f"æ€»é‚®ä»¶æ•°é‡: {len(newsgroups_train.data) + len(newsgroups_test.data)}")
    print(f"è®­ç»ƒé›†æ•°é‡: {len(newsgroups_train.data)}")
    print(f"æµ‹è¯•é›†æ•°é‡: {len(newsgroups_test.data)}")
    print(f"ç±»åˆ«åç§°: {newsgroups_train.target_names}")
    print()
    
    # TF-IDFç‰¹å¾åŒ–
    vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')
    X_train_tfidf = vectorizer.fit_transform(newsgroups_train.data)
    X_test_tfidf = vectorizer.transform(newsgroups_test.data)
    
    print(f"ğŸ”¤ TF-IDFç‰¹å¾åŒ–ç»“æœ:")
    print(f"è®­ç»ƒé›†ç‰¹å¾ç»´åº¦: {X_train_tfidf.shape}")
    print(f"æµ‹è¯•é›†ç‰¹å¾ç»´åº¦: {X_test_tfidf.shape}")
    print(f"ç‰¹å¾ç¨€ç–åº¦: {(1 - X_train_tfidf.nnz / (X_train_tfidf.shape[0] * X_train_tfidf.shape[1])) * 100:.2f}%")
    print()
    
    # è®­ç»ƒSVMåˆ†ç±»å™¨
    svm_classifier = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)
    svm_classifier.fit(X_train_tfidf, newsgroups_train.target)
    
    # é¢„æµ‹
    y_pred = svm_classifier.predict(X_test_tfidf)
    
    # è®¡ç®—å‡†ç¡®ç‡
    accuracy = accuracy_score(newsgroups_test.target, y_pred)
    print(f"ğŸ“Š åƒåœ¾é‚®ä»¶åˆ†ç±»ç»“æœ:")
    print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.4f}")
    print()
    
    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    print(f"ğŸ“ˆ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(newsgroups_test.target, y_pred, target_names=newsgroups_train.target_names))
    
    return {
        'accuracy': accuracy,
        'train_samples': len(newsgroups_train.data),
        'test_samples': len(newsgroups_test.data)
    }

if __name__ == "__main__":
    print("å¼€å§‹SVMå®éªŒæµ‹è¯•...\n")
    
    # æµ‹è¯•çº¿æ€§SVM
    linear_results = test_linear_svm()
    
    # æµ‹è¯•éçº¿æ€§SVM
    nonlinear_results = test_nonlinear_svm()
    
    # æµ‹è¯•å‚æ•°ä¼˜åŒ–
    optimization_results = test_parameter_optimization()
    
    # æµ‹è¯•åƒåœ¾é‚®ä»¶åˆ†ç±»
    spam_results = test_spam_classification()
    
    print("\n=== å®éªŒæ€»ç»“ ===")
    print(f"çº¿æ€§SVMå‡†ç¡®ç‡: {linear_results['accuracy']:.4f}")
    print(f"æœ€ä½³éçº¿æ€§æ ¸å‡½æ•°: {nonlinear_results['best_kernel']} (å‡†ç¡®ç‡: {nonlinear_results['best_accuracy']:.4f})")
    print(f"å‚æ•°ä¼˜åŒ–åå‡†ç¡®ç‡: {optimization_results['test_accuracy']:.4f}")
    print(f"åƒåœ¾é‚®ä»¶åˆ†ç±»å‡†ç¡®ç‡: {spam_results['accuracy']:.4f}")