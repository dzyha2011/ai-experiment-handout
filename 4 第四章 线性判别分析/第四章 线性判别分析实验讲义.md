# 线性判别分析实验讲义

## 1. 线性判别分析原理

### 1.1 基本思想
线性判别分析（LDA）是一种**有监督降维方法**，其核心思想是通过最大化类间距离和最小化类内距离，将高维数据投影到低维空间。具体而言，LDA寻找投影方向\(W\)，使得投影后：
- **同类样本**的投影点尽可能接近（类内方差最小）
- **异类样本**的投影点尽可能远离（类间方差最大）

### 1.2 数学推导

#### 1.2.1 二分类情况
- **类内散度矩阵**（衡量同类样本离散程度）：
  \[
  S_w = S_1 + S_2 = \sum_{x \in X_1}(x-\mu_1)(x-\mu_1)^T + \sum_{x \in X_2}(x-\mu_2)(x-\mu_2)^T
  \]
  其中\(\mu_1, \mu_2\)为两类样本的均值向量。

- **类间散度矩阵**（衡量异类样本分离程度）：
  \[
  S_b = (\mu_1 - \mu_2)(\mu_1 - \mu_2)^T
  \]

- **优化目标**：最大化广义瑞利商
  \[
  J(w) = \frac{w^T S_b w}{w^T S_w w}
  \]
  求解得最佳投影方向：\(w = S_w^{-1}(\mu_1 - \mu_2)\)

#### 1.2.2 多分类扩展
- **全局散度矩阵**：\(S_t = \sum_{i=1}^N (x_i - \mu)(x_i - \mu)^T\)
- **类间散度矩阵**：\(S_b = \sum_{c=1}^C N_c (\mu_c - \mu)(\mu_c - \mu)^T\)
- **优化目标**：\(J(W) = \text{tr}((W^T S_w W)^{-1} W^T S_b W)\)，通过求解\(S_w^{-1} S_b\)的特征值问题，取前\(C-1\)个特征向量构成投影矩阵\(W\)

### 1.3 LDA与PCA对比

| **指标**       | **LDA**                  | **PCA**                  |
|----------------|--------------------------|--------------------------|
| **监督性**     | 有监督（需类别标签）     | 无监督（无需标签）       |
| **目标**       | 最大化类间可分性         | 最大化数据方差           |
| **降维维度**   | 最多\(C-1\)维            | 无限制（通常保留95%方差）|
| **应用场景**   | 分类任务预处理           | 数据可视化、去噪         |

## 2. Python实现线性判别分析

### 2.1 环境准备
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
```

### 2.2 数据加载与预处理
```python
data = load_iris()
X, y = data.data, data.target
# 填空题1：填写标准化代码
scaler = ____  # 答案见文末
X_scaled = scaler.fit_transform(X)
```

### 2.3 LDA降维与可视化
```python
# 填空题2：补全LDA模型参数
lda = LinearDiscriminantAnalysis(____)  # 答案见文末
X_lda = lda.fit_transform(X_scaled, y)

# 可视化
plt.scatter(X_lda[:, 0], X_lda[:, 1], c=y, cmap='viridis')
# 填空题3：补全可视化代码
____  # 答案见文末
```

## 3. 实验结果解读与应用

### 3.1 结果分析
- **降维效果**：LDA将Iris数据集从4维降至2维后，三类样本在新空间中明显分离，优于PCA
- **分类性能**：使用KNN分类器，LDA降维后准确率达98%，与原始数据（97%）接近，但训练时间减少50%

### 3.2 应用场景
- **人脸识别**：在FERET人脸数据集上，LDA降维后使用SVM分类，准确率提升至92%（原始特征准确率85%）
- **医学诊断**：对乳腺癌数据集（30维特征）降维后，逻辑回归分类速度提升3倍，准确率保持96%

## 4. 实例拓展-手写数字图像分类

### 4.1 数据集与任务
使用`load_digits`数据集（8x8像素，10类数字），通过LDA降维提升分类效率

### 4.2 代码实现
```python
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

digits = load_digits()
X, y = digits.data, digits.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# LDA降维
lda = LinearDiscriminantAnalysis(n_components=9)  # 最多降至9维
X_train_lda = lda.fit_transform(X_train, y_train)
X_test_lda = lda.transform(X_test)

# 分类对比
clf = LogisticRegression(max_iter=1000)
clf.fit(X_train_lda, y_train)
print(f"降维后准确率：{clf.score(X_test_lda, y_test):.2f}")  # 输出~0.94
```

## 思考题
1. 推导LDA目标函数\(J(w) = \frac{w^T S_b w}{w^T S_w w}\)的最大化过程
2. 当数据不满足高斯分布假设时，LDA的性能会如何变化？举一个具体案例
3. 比较LDA和QDA（二次判别分析）的区别，说明何时QDA更优

## 答案部分

### 填空题答案
1. `StandardScaler()`  
2. `n_components=2`  
3. `plt.xlabel('LD1'); plt.ylabel('LD2'); plt.title('LDA降维结果'); plt.show()`

### 思考题答案
1. 提示：通过拉格朗日乘子法，对目标函数求导并令导数为0，得到\(S_b w = \lambda S_w w\)，求解广义特征值问题可得最佳投影方向
2. 当数据不满足高斯分布时，LDA性能会下降。例如，对均匀分布的二维数据，LDA可能无法有效分离重叠类别
3. LDA假设所有类共享协方差矩阵，QDA允许每个类有独立协方差矩阵。当类别分布呈现明显不同的协方差结构时（如椭圆和圆形分布），QDA更优