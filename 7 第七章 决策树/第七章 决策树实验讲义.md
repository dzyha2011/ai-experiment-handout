# 人工智能课程实验讲义：决策树实验


## 1. 决策树算法原理

### 1.1 基本概念
决策树（Decision Tree）是一种基于树状结构进行决策的监督学习模型，通过递归划分特征空间实现对目标变量的预测。其核心思想是**“分而治之”**：从根节点开始，根据特征的最优划分准则（如信息增益、基尼系数）将数据集分裂为子集，直至子集中样本属于同一类别（分类）或目标值趋于稳定（回归）。


### 1.2 决策树学习过程
决策树的构建分为三个阶段：
1. **特征选择**：通过划分准则选择最优特征进行分裂；
2. **树的生成**：递归分裂节点，直至满足停止条件（如最大深度、最小样本数）；
3. **剪枝**：移除过拟合的分支，提升模型泛化能力。


### 1.3 特征选择准则
#### 1.3.1 信息熵与信息增益（ID3算法）
- **信息熵（Entropy）**：衡量数据集的不确定性，公式为：  
  $$H(D) = -\sum_{k=1}^{K} p_k \log_2 p_k$$  
  其中 $p_k$ 为第 $k$ 类样本在数据集 $D$ 中的占比。熵值越高，数据越混乱。
- **信息增益（Information Gain）**：特征 $A$ 分裂后熵的减少量，公式为：  
  $$Gain(D,A) = H(D) - \sum_{v=1}^{V} \frac{|D_v|}{|D|} H(D_v)$$  
  其中 $D_v$ 为特征 $A$ 取值为 $v$ 的子集。ID3算法选择信息增益最大的特征进行分裂。

#### 1.3.2 信息增益率（C4.5算法）
为解决信息增益偏向多取值特征的问题，C4.5使用**信息增益率**：  
$$Gain\_ratio(D,A) = \frac{Gain(D,A)}{IV(A)}$$  
其中 $IV(A) = -\sum_{v=1}^{V} \frac{|D_v|}{|D|} \log_2 \frac{|D_v|}{|D|}$ 为特征 $A$ 的固有值，取值越多，$IV(A)$ 越大，增益率被惩罚。

#### 1.3.3 基尼系数（CART算法）
基尼系数（Gini Impurity）衡量样本被随机分类的错误概率，公式为：  
$$Gini(D) = 1 - \sum_{k=1}^{K} p_k^2$$  
CART（分类与回归树）算法选择基尼系数最小的特征分裂，适用于分类问题；回归问题则采用**均方误差（MSE）** 作为划分准则：  
$$MSE(D) = \frac{1}{|D|} \sum_{i \in D} (y_i - \bar{y})^2$$  


### 1.4 决策树生成算法对比
| 算法 | 划分准则 | 树结构 | 适用任务 | 处理连续值 | 处理缺失值 |
|------|----------|--------|----------|------------|------------|
| ID3  | 信息增益 | 多叉树 | 分类     | 不支持     | 不支持     |
| C4.5 | 信息增益率 | 多叉树 | 分类     | 支持（离散化） | 支持（加权处理） |
| CART | 基尼系数（分类）/MSE（回归） | 二叉树 | 分类/回归 | 支持（寻找最优切分点） | 支持（代理分裂） |


## 2. 决策树的构建与可视化

### 2.1 构建步骤
以CART分类树为例，构建流程如下：
1. **输入**：训练集 $D$，特征集 $A$，停止条件（如 $max\_depth=5$）；
2. **若 $D$ 中样本属于同一类别或达到停止条件**：返回叶节点（类别为样本多数类）；
3. **否则**：计算所有特征的基尼系数，选择最优特征 $A^*$ 及切分点 $t^*$；
4. **按 $A^* \leq t^*$ 和 $A^* > t^*$ 分裂为左右子树**，递归执行步骤2-3；
5. **输出**：决策树 $T$。


### 2.2 可视化工具与方法
#### 2.2.1 基础可视化：`sklearn.tree.plot_tree`
适用于快速展示树结构，无需额外安装工具。  
**代码示例**：
```python
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt

# 训练模型
iris = load_iris()
clf = DecisionTreeClassifier(max_depth=3, random_state=42)
clf.fit(iris.data, iris.target)

# 可视化决策树
plt.figure(figsize=(15, 8))
plot_tree(
    clf,
    feature_names=iris.feature_names,
    class_names=iris.target_names,
    filled=True,  # 按类别填充颜色
    rounded=True,  # 圆角矩形
    impurity=True  # 显示不纯度（基尼系数）
)
plt.title("鸢尾花数据集决策树（max_depth=3）")
plt.show()
```

#### 2.2.2 高级可视化：`graphviz` + `pydotplus`
生成高清树结构文件（如PDF/PNG），需在官网下载相应的([Graphviz](https://graphviz.org/download/))软件在本地安装（需要将安装路径的bin文件夹添加到系统环境变量），然后再安装：  
```bash
pip install graphviz pydotplus
```
**代码示例**：

```python
from sklearn.tree import export_graphviz
import pydotplus
from IPython.display import Image

dot_data = export_graphviz(
    clf,
    out_file=None,
    feature_names=iris.feature_names,
    class_names=iris.target_names,
    filled=True,
    impurity=True,
    special_characters=True
)
graph = pydotplus.graph_from_dot_data(dot_data)
graph.write_png("iris_tree.png")  # 保存为PNG
Image(graph.create_png())  # 在Notebook中显示
```


## 3. Python 实现决策树分类与回归

### 3.1 决策树分类（鸢尾花数据集）
#### 3.1.1 数据加载与探索
**数据集**：鸢尾花（Iris）数据集，含4个特征（花萼/花瓣长度、宽度），3类鸢尾花。  
```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# 加载数据
iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42  # 80%训练，20%测试
)
print(f"特征名称：{iris.feature_names}")
print(f"训练集样本数：{X_train.shape[0]}, 测试集样本数：{X_test.shape[0]}")
```

#### 3.1.2 模型训练与评估（含代码填空）
```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix

# 模型初始化（填空1：选择划分准则，填空2：设置最大深度）
clf = DecisionTreeClassifier(
    criterion=___①___,  # 可选 'gini' 或 'entropy'
    max_depth=___②___,  # 建议填写 3
    random_state=42
)

# 训练模型
clf.fit(X_train, y_train)

# 预测与评估
y_pred = clf.predict(X_test)
acc = accuracy_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

print(f"测试集准确率：{acc:.4f}")
print("混淆矩阵：\n", cm)
```

#### 3.1.3 分类结果可视化
**混淆矩阵热图**：
```python
import seaborn as sns

plt.figure(figsize=(8, 6))
sns.heatmap(
    cm,
    annot=True,  # 显示数值
    fmt="d",  # 整数格式
    cmap="Blues",
    xticklabels=iris.target_names,
    yticklabels=iris.target_names
)
plt.xlabel("预测类别")
plt.ylabel("真实类别")
plt.title("决策树分类混淆矩阵")
plt.show()
```


### 3.2 决策树回归（California房价数据集）
#### 3.2.1 数据加载与探索
**数据集**：California房价数据集，含8个特征（平均收入、房龄等），目标为房价中位数（单位：万美元）。  
```python
from sklearn.datasets import fetch_california_housing

# 加载数据
# fetch_california_housing()函数从sklearn内置数据集中加载加州房价数据
# 该数据集包含20640个样本，每个样本有8个特征
housing = fetch_california_housing()
X, y = housing.data, housing.target  # X为特征矩阵，y为目标变量（房价）

# 数据集划分：80%用于训练，20%用于测试
# test_size=0.2表示测试集占20%，random_state=42确保结果可重现
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 输出数据集基本信息
print(f"特征名称：{housing.feature_names}")
print(f"训练集样本数：{X_train.shape[0]}, 测试集样本数：{X_test.shape[0]}")
print(f"特征维度：{X_train.shape[1]}")
print(f"目标变量范围：{y.min():.2f}~{y.max():.2f}万美元")

# 查看特征的统计信息，了解数据分布
import pandas as pd
df_info = pd.DataFrame(X_train, columns=housing.feature_names)
print("\n特征统计信息：")
print(df_info.describe())
```

#### 3.2.2 模型训练与评估（含代码填空）
```python
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# 模型初始化（填空3：设置最小叶节点样本数）
# DecisionTreeRegressor用于回归任务，与分类树的主要区别：
# 1. 使用MSE（均方误差）作为分裂准则，而非基尼系数或信息熵
# 2. 叶节点预测值为该节点样本的平均值，而非多数类别
reg = DecisionTreeRegressor(
    criterion="squared_error",  # MSE准则：最小化预测值与真实值的平方误差
    min_samples_leaf=___③___,  # 叶节点最小样本数，防止过拟合（建议填写 5）
    random_state=42  # 随机种子，确保结果可重现
)

# 模型训练：通过递归分裂构建回归树
# 算法会寻找最优特征和切分点，使得分裂后子节点的MSE最小
print("开始训练决策树回归模型...")
reg.fit(X_train, y_train)
print(f"训练完成！树的深度：{reg.get_depth()}")
print(f"叶节点数量：{reg.get_n_leaves()}")

# 模型预测：对测试集进行预测
# 对于每个测试样本，从根节点开始，根据特征值沿着树路径到达叶节点
# 叶节点的平均值即为预测结果
y_pred = reg.predict(X_test)

# 评估指标计算与解释
mse = mean_squared_error(y_test, y_pred)  # 均方误差：预测误差的平方的平均值
rmse = np.sqrt(mse)  # 均方根误差：MSE的平方根，单位与目标变量相同
r2 = r2_score(y_test, y_pred)  # 决定系数：解释方差的比例，1表示完美预测

print(f"\n模型评估结果：")
print(f"测试集MSE：{mse:.4f}")
print(f"测试集RMSE：{rmse:.4f} 万美元")  # RMSE表示平均预测误差
print(f"测试集R²：{r2:.4f}")  # R²越接近1，模型拟合效果越好
print(f"平均绝对误差：{np.mean(np.abs(y_test - y_pred)):.4f} 万美元")
```

#### 3.2.3 回归结果可视化
**预测值 vs 真实值散点图**：
```python
import matplotlib.pyplot as plt

# 创建预测值与真实值的散点图
# 理想情况下，所有点都应该落在y=x直线上（完美预测）
plt.figure(figsize=(10, 8))
plt.scatter(y_test, y_pred, alpha=0.6, s=30)  # alpha控制透明度，s控制点的大小

# 绘制理想预测线（y=x）：红色虚线表示完美预测的情况
plt.plot([y.min(), y.max()], [y.min(), y.max()], "r--", lw=2, label="理想预测线")

# 计算并绘制拟合线：实际预测的趋势线
z = np.polyfit(y_test, y_pred, 1)  # 一次多项式拟合
p = np.poly1d(z)
plt.plot(y_test, p(y_test), "b-", alpha=0.8, label=f"拟合线 (斜率={z[0]:.3f})")

plt.xlabel("真实房价（万美元）", fontsize=12)
plt.ylabel("预测房价（万美元）", fontsize=12)
plt.title(f"决策树回归：预测值 vs 真实值\n(R² = {r2:.4f}, RMSE = {rmse:.4f})", fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# 分析：点越接近红色虚线，预测越准确
# 如果点分布在直线两侧且较为均匀，说明模型没有系统性偏差
```

**残差分析图**：
```python
import seaborn as sns

# 残差 = 真实值 - 预测值，用于检验模型的预测质量
residuals = y_test - y_pred

# 创建包含多个子图的残差分析
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# 1. 残差分布直方图：检查残差是否服从正态分布
axes[0, 0].hist(residuals, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
axes[0, 0].axvline(residuals.mean(), color='red', linestyle='--', 
                   label=f'均值={residuals.mean():.4f}')
axes[0, 0].set_xlabel("残差（真实值-预测值）")
axes[0, 0].set_ylabel("频数")
axes[0, 0].set_title("残差分布直方图")
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# 2. 残差密度图：更平滑地显示残差分布
sns.histplot(residuals, kde=True, bins=30, ax=axes[0, 1])
axes[0, 1].set_xlabel("残差")
axes[0, 1].set_ylabel("密度")
axes[0, 1].set_title("残差密度分布")
axes[0, 1].grid(True, alpha=0.3)

# 3. 残差 vs 预测值：检查异方差性（残差方差是否恒定）
axes[1, 0].scatter(y_pred, residuals, alpha=0.6)
axes[1, 0].axhline(y=0, color='red', linestyle='--')  # 零残差线
axes[1, 0].set_xlabel("预测值")
axes[1, 0].set_ylabel("残差")
axes[1, 0].set_title("残差 vs 预测值")
axes[1, 0].grid(True, alpha=0.3)

# 4. Q-Q图：检验残差是否服从正态分布
from scipy import stats
stats.probplot(residuals, dist="norm", plot=axes[1, 1])
axes[1, 1].set_title("残差Q-Q图（正态性检验）")
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# 残差分析结论：
print(f"\n残差分析结果：")
print(f"残差均值：{residuals.mean():.6f} (接近0表示无偏)")
print(f"残差标准差：{residuals.std():.4f}")
print(f"残差范围：[{residuals.min():.4f}, {residuals.max():.4f}]")

# 理想情况：残差应该随机分布在0附近，无明显模式
# 如果残差呈现某种模式，可能表明模型存在系统性偏差
```


## 4. 决策树剪枝优化实验

### 4.1 剪枝概述
决策树剪枝是防止过拟合的重要技术，分为**预剪枝**和**后剪枝**两类：
- **预剪枝（Pre-pruning）**：在树构建过程中提前停止分裂
- **后剪枝（Post-pruning）**：先构建完整树，再移除不必要的分支

### 4.2 预剪枝（Pre-pruning）
#### 4.2.1 算法原理
预剪枝通过设置停止条件，在树构建过程中提前终止节点分裂，避免生成过于复杂的树。

**算法步骤**：
1. **初始化**：从根节点开始，包含所有训练样本
2. **检查停止条件**：对当前节点，检查是否满足以下任一条件：
   - 节点样本数 < `min_samples_split`（最小分裂样本数）
   - 分裂后子节点样本数 < `min_samples_leaf`（最小叶节点样本数）
   - 树深度 ≥ `max_depth`（最大深度）
   - 节点纯度已达到阈值（如基尼系数 < `min_impurity_decrease`）
   - 信息增益 < `min_impurity_decrease`（最小不纯度减少量）
3. **若满足停止条件**：将当前节点设为叶节点，预测值为该节点样本的多数类（分类）或平均值（回归）
4. **若不满足停止条件**：
   - 计算所有特征的分裂准则（基尼系数、信息增益等）
   - 选择最优特征和切分点进行分裂
   - 递归对左右子节点执行步骤2-4
5. **输出**：预剪枝决策树

**优点**：计算效率高，避免构建不必要的分支  
**缺点**：可能过早停止，错过后续有价值的分裂

#### 4.2.2 实验设计：不同预剪枝参数对性能的影响
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier

# 实验1：max_depth（最大深度）的影响
print("=== 实验1：max_depth对模型性能的影响 ===")
depths = [1, 2, 3, 4, 5, 8, 10, None]  # None表示不限制深度
train_scores_depth = []
test_scores_depth = []
tree_sizes = []  # 记录树的大小（叶节点数）

for d in depths:
    clf = DecisionTreeClassifier(max_depth=d, random_state=42)
    clf.fit(X_train, y_train)
    
    train_score = clf.score(X_train, y_train)
    test_score = clf.score(X_test, y_test)
    tree_size = clf.get_n_leaves()
    
    train_scores_depth.append(train_score)
    test_scores_depth.append(test_score)
    tree_sizes.append(tree_size)
    
    print(f"max_depth={d}: 训练准确率={train_score:.4f}, "
          f"测试准确率={test_score:.4f}, 叶节点数={tree_size}")

# 可视化max_depth实验结果
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

# 准确率曲线
depth_labels = [str(d) if d is not None else "无限制" for d in depths]
ax1.plot(range(len(depths)), train_scores_depth, "o-", label="训练集准确率", linewidth=2)
ax1.plot(range(len(depths)), test_scores_depth, "s-", label="测试集准确率", linewidth=2)
ax1.set_xticks(range(len(depths)))
ax1.set_xticklabels(depth_labels, rotation=45)
ax1.set_xlabel("max_depth")
ax1.set_ylabel("准确率")
ax1.set_title("预剪枝：max_depth对准确率的影响")
ax1.legend()
ax1.grid(True, alpha=0.3)

# 树大小变化
ax2.plot(range(len(depths)), tree_sizes, "^-", color="green", linewidth=2)
ax2.set_xticks(range(len(depths)))
ax2.set_xticklabels(depth_labels, rotation=45)
ax2.set_xlabel("max_depth")
ax2.set_ylabel("叶节点数")
ax2.set_title("预剪枝：max_depth对树大小的影响")
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# 实验2：min_samples_leaf（最小叶节点样本数）的影响
print("\n=== 实验2：min_samples_leaf对模型性能的影响 ===")
min_samples_values = [1, 2, 5, 10, 20, 50]
train_scores_samples = []
test_scores_samples = []

for min_samples in min_samples_values:
    clf = DecisionTreeClassifier(min_samples_leaf=min_samples, random_state=42)
    clf.fit(X_train, y_train)
    
    train_score = clf.score(X_train, y_train)
    test_score = clf.score(X_test, y_test)
    
    train_scores_samples.append(train_score)
    test_scores_samples.append(test_score)
    
    print(f"min_samples_leaf={min_samples}: 训练准确率={train_score:.4f}, "
          f"测试准确率={test_score:.4f}")

# 可视化min_samples_leaf实验结果
plt.figure(figsize=(10, 6))
plt.plot(min_samples_values, train_scores_samples, "o-", label="训练集准确率", linewidth=2)
plt.plot(min_samples_values, test_scores_samples, "s-", label="测试集准确率", linewidth=2)
plt.xlabel("min_samples_leaf")
plt.ylabel("准确率")
plt.title("预剪枝：min_samples_leaf对模型性能的影响")
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```
**预期结果分析**：
- 随着 `max_depth` 增加，训练集准确率持续上升（过拟合），测试集准确率先升后降
- 最优深度通常在3~5之间，平衡了模型复杂度和泛化能力
- `min_samples_leaf` 增加时，模型变简单，可能出现欠拟合

### 4.3 后剪枝（Post-pruning）
#### 4.3.1 算法原理
后剪枝先构建完整的决策树，再通过自底向上的方式移除不必要的分支。最常用的是**成本复杂度剪枝（Cost Complexity Pruning, CCP）**。

**成本复杂度剪枝算法步骤**：

1. **构建完整树**：不设置停止条件，构建完整的决策树 $T_0$

2. **定义成本复杂度函数**：
   $$R_\alpha(T) = R(T) + \alpha \cdot |T|$$
   其中：
   - $R(T)$：树 $T$ 的训练误差（分类错误率或MSE）
   - $|T|$：树 $T$ 的叶节点数（复杂度惩罚项）
   - $\alpha$：复杂度参数（$\alpha \geq 0$）

3. **计算每个内部节点的剪枝收益**：
   对于内部节点 $t$，计算剪枝前后的成本变化：
   $$g(t) = \frac{R(t) - R(T_t)}{|T_t| - 1}$$
   其中：
   - $R(t)$：将节点 $t$ 剪枝为叶节点的误差
   - $R(T_t)$：以 $t$ 为根的子树的误差
   - $|T_t|$：以 $t$ 为根的子树的叶节点数

4. **选择最小剪枝收益的节点**：
   $$\alpha_1 = \min_{t \in T_0} g(t)$$
   剪枝所有满足 $g(t) = \alpha_1$ 的节点，得到新树 $T_1$

5. **重复剪枝过程**：
   对 $T_1$ 重复步骤3-4，得到 $T_2, T_3, \ldots$，直到只剩根节点

6. **交叉验证选择最优 $\alpha$**：
   使用验证集评估不同 $\alpha$ 值对应的树，选择泛化性能最好的 $\alpha^*$

7. **输出最优树**：返回对应 $\alpha^*$ 的剪枝树

**优点**：能找到全局最优的剪枝方案，泛化性能通常更好  
**缺点**：计算复杂度高，需要先构建完整树

#### 4.3.2 实验设计：成本复杂度剪枝
```python
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt

print("=== 后剪枝实验：成本复杂度剪枝 ===")

# 步骤1：构建完整树并获取剪枝路径
clf_full = DecisionTreeClassifier(random_state=42)
clf_full.fit(X_train, y_train)

print(f"完整树信息：深度={clf_full.get_depth()}, 叶节点数={clf_full.get_n_leaves()}")

# 获取成本复杂度剪枝路径
# cost_complexity_pruning_path返回不同alpha值及对应的不纯度
path = clf_full.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas = path.ccp_alphas[:-1]  # 排除使树为空的最后一个alpha
impurities = path.impurities[:-1]

print(f"剪枝路径包含{len(ccp_alphas)}个alpha值")
print(f"Alpha范围：{ccp_alphas[0]:.6f} ~ {ccp_alphas[-1]:.6f}")

# 步骤2：为每个alpha训练剪枝后的树
clfs = []
train_scores = []
test_scores = []
tree_depths = []
tree_leaves = []

print("\n训练不同alpha值的剪枝树...")
for i, alpha in enumerate(ccp_alphas):
    clf = DecisionTreeClassifier(ccp_alpha=alpha, random_state=42)
    clf.fit(X_train, y_train)
    clfs.append(clf)
    
    train_score = clf.score(X_train, y_train)
    test_score = clf.score(X_test, y_test)
    
    train_scores.append(train_score)
    test_scores.append(test_score)
    tree_depths.append(clf.get_depth())
    tree_leaves.append(clf.get_n_leaves())
    
    if i % max(1, len(ccp_alphas)//10) == 0:  # 每10%输出一次进度
        print(f"Alpha={alpha:.6f}: 训练准确率={train_score:.4f}, "
              f"测试准确率={test_score:.4f}, 深度={clf.get_depth()}, "
              f"叶节点数={clf.get_n_leaves()}")

# 步骤3：寻找最优alpha
best_idx = np.argmax(test_scores)
best_alpha = ccp_alphas[best_idx]
best_test_score = test_scores[best_idx]

print(f"\n最优剪枝结果：")
print(f"最优alpha：{best_alpha:.6f}")
print(f"最优测试准确率：{best_test_score:.4f}")
print(f"对应树深度：{tree_depths[best_idx]}")
print(f"对应叶节点数：{tree_leaves[best_idx]}")

# 步骤4：可视化剪枝效果
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))

# 1. 准确率 vs alpha
ax1.plot(ccp_alphas, train_scores, "o-", label="训练集准确率", alpha=0.8)
ax1.plot(ccp_alphas, test_scores, "s-", label="测试集准确率", alpha=0.8)
ax1.axvline(best_alpha, color='red', linestyle='--', alpha=0.7, label=f'最优alpha={best_alpha:.4f}')
ax1.set_xlabel("ccp_alpha")
ax1.set_ylabel("准确率")
ax1.set_title("后剪枝：准确率 vs Alpha")
ax1.legend()
ax1.grid(True, alpha=0.3)

# 2. 树复杂度 vs alpha
ax2.plot(ccp_alphas, tree_depths, "^-", label="树深度", color="green")
ax2.plot(ccp_alphas, tree_leaves, "v-", label="叶节点数", color="orange")
ax2.axvline(best_alpha, color='red', linestyle='--', alpha=0.7)
ax2.set_xlabel("ccp_alpha")
ax2.set_ylabel("树复杂度")
ax2.set_title("后剪枝：树复杂度 vs Alpha")
ax2.legend()
ax2.grid(True, alpha=0.3)

# 3. 不纯度 vs alpha
ax3.plot(ccp_alphas, impurities, "d-", color="purple")
ax3.axvline(best_alpha, color='red', linestyle='--', alpha=0.7)
ax3.set_xlabel("ccp_alpha")
ax3.set_ylabel("总不纯度")
ax3.set_title("后剪枝：总不纯度 vs Alpha")
ax3.grid(True, alpha=0.3)

# 4. 训练集与测试集准确率差异
accuracy_gap = np.array(train_scores) - np.array(test_scores)
ax4.plot(ccp_alphas, accuracy_gap, "o-", color="red", label="过拟合程度")
ax4.axvline(best_alpha, color='red', linestyle='--', alpha=0.7)
ax4.axhline(0, color='black', linestyle='-', alpha=0.3)
ax4.set_xlabel("ccp_alpha")
ax4.set_ylabel("训练准确率 - 测试准确率")
ax4.set_title("后剪枝：过拟合程度 vs Alpha")
ax4.legend()
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# 步骤5：比较剪枝前后的树结构
print(f"\n剪枝效果对比：")
print(f"原始树：深度={clf_full.get_depth()}, 叶节点数={clf_full.get_n_leaves()}, "
      f"测试准确率={clf_full.score(X_test, y_test):.4f}")
print(f"最优剪枝树：深度={tree_depths[best_idx]}, 叶节点数={tree_leaves[best_idx]}, "
      f"测试准确率={best_test_score:.4f}")
print(f"复杂度减少：{(1 - tree_leaves[best_idx]/clf_full.get_n_leaves())*100:.1f}%")
```

### 4.4 预剪枝 vs 后剪枝对比
| 特征 | 预剪枝 | 后剪枝 |
|------|--------|--------|
| **计算复杂度** | 低（边构建边剪枝） | 高（需构建完整树） |
| **剪枝精度** | 可能过早停止 | 全局最优剪枝 |
| **内存占用** | 小（树较小） | 大（需存储完整树） |
| **泛化性能** | 一般 | 通常更好 |
| **参数调节** | 多个参数需调节 | 主要调节alpha |
| **适用场景** | 大数据集、实时应用 | 小中型数据集、离线训练 |

**实践建议**：
- 数据量大、计算资源有限：优先使用预剪枝
- 追求最佳性能、计算资源充足：使用后剪枝
- 可以结合使用：先预剪枝控制树大小，再后剪枝精细调优


## 5. 实例拓展-电商用户购买行为分析

### 5.1 项目背景与商业价值
#### 5.1.1 业务背景
在电商平台中，**用户购买行为预测**是核心的商业智能应用之一。通过分析用户的浏览、点击、收藏、加购物车等行为轨迹，预测用户是否会最终完成购买，对电商平台具有重要的商业价值：

**商业应用场景**：
1. **精准营销**：识别高购买意向用户，进行定向推送和优惠券投放
2. **库存管理**：预测商品需求，优化库存配置和供应链管理
3. **个性化推荐**：基于购买概率调整推荐算法权重
4. **用户分层**：区分潜在买家和浏览用户，制定差异化运营策略
5. **转化率优化**：识别影响购买的关键因素，优化用户体验

#### 5.1.2 数据集介绍
**数据来源**：阿里巴巴提供的淘宝用户行为数据集（UserBehavior Dataset），这是一个包含约1亿条用户行为记录的大规模真实电商数据集。

**数据集特点**：
- **时间跨度**：2017年11月25日至2017年12月3日（9天）
- **用户规模**：约100万活跃用户
- **商品规模**：约400万商品，涵盖9000+商品类别
- **行为类型**：点击(pv)、购买(buy)、加购物车(cart)、收藏(fav)
- **数据质量**：经过清洗的真实用户行为，无重复和异常记录

**数据集下载**：

- **官方地址**：[阿里云天池数据集](https://tianchi-jupyter-sh.oss-cn-shanghai.aliyuncs.com/file/opensearch/documents/649/UserBehavior.csv.zip?Expires=1754516991&OSSAccessKeyId=STS.NXxhWuaGvP9JMbdb2879BCcw1&Signature=t4UO8xDvxZE%2Fdp01HXGfr4X6V64%3D&response-content-disposition=attachment%3B%20&security-token=CAIS0wJ1q6Ft5B2yfSjIr5vNI%2B3BjJhX5%2FuhT0TVhjJtO7ZurKbc0zz2IHhMeXdvCeAfsvs1lG9W7vYYlrp6SJtIXleCZtF94oxN9h2gb4fb41I5B17k08%2FLI3OaLjKm9u2wCryLYbGwU%2FOpbE%2B%2B5U0X6LDmdDKkckW4OJmS8%2FBOZcgWWQ%2FKBlgvRq0hRG1YpdQdKGHaONu0LxfumRCwNkdzvRdmgm4NgsbWgO%2Fks0aH1Q2rlbdM%2F9WvfMX0MvMBZskvD42Hu8VtbbfE3SJq7BxHybx7lqQs%2B02c5onAXQYBs0zabLCErYM3fFVjGKE5H6Nft73wheU9sevOjZ%2F6jg5XOu1FguwGsiZLaaEuccPe1bZRHd6TUxylWUAROTgCrQht5%2F5PgYmu1w69ztNUSEVKZHizAdSM0zPpeWyIIIvei5pV6vcbgiuuk5PkSFbnP23%2BAnpwUvcagAGB3qXHo4%2F45Gq%2BZhU0SPLV%2B7p%2BSieCvUIdUn1itVAZqg5FVRFjRujqGQNkAV6gSp0UWST9GgjUvQg7y4reUhCv1O7HJT6RICJR6Gj6FHJZoetJ0BOx8F%2FgTnyizEC5irmy8f2leFFP%2BJUFHuFgBp%2FT6pG2t8tHtq7%2BoXMcC5fVISAA)

**数据字段说明**：
| 字段名 | 数据类型 | 说明 | 示例值 |
|--------|----------|------|--------|
| `user_id` | int | 用户唯一标识符 | 1001 |
| `item_id` | int | 商品唯一标识符 | 2268318 |
| `category_id` | int | 商品类别ID | 2520377 |
| `behavior` | string | 用户行为类型 | pv, buy, cart, fav |
| `timestamp` | int | 行为发生时间戳 | 1511544070 |

**行为类型分布**（基于完整数据集）：
- **pv（点击浏览）**：约89.7%，用户浏览商品详情页
- **cart（加购物车）**：约5.7%，用户将商品加入购物车
- **fav（收藏）**：约2.8%，用户收藏商品
- **buy（购买）**：约1.8%，用户完成购买行为

### 5.2 数据预处理与特征工程
```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns

# 数据加载（由于数据集较大，这里使用采样数据进行演示）
print("=== 加载淘宝用户行为数据集 ===")
# 注意：完整数据集约4GB，建议先用采样数据进行实验
df = pd.read_csv(
    "UserBehavior.csv",
    names=["user_id", "item_id", "category_id", "behavior", "timestamp"],
    nrows=1000000  # 采样100万条记录进行演示
)

print(f"数据集基本信息：")
print(f"- 总记录数：{len(df):,}")
print(f"- 用户数：{df['user_id'].nunique():,}")
print(f"- 商品数：{df['item_id'].nunique():,}")
print(f"- 商品类别数：{df['category_id'].nunique():,}")

# 行为类型分布分析
behavior_counts = df['behavior'].value_counts()
print(f"\n行为类型分布：")
for behavior, count in behavior_counts.items():
    percentage = count / len(df) * 100
    print(f"- {behavior}: {count:,} ({percentage:.2f}%)")

# 可视化行为分布
plt.figure(figsize=(12, 5))

# 行为类型饼图
plt.subplot(1, 2, 1)
plt.pie(behavior_counts.values, labels=behavior_counts.index, autopct='%1.1f%%', startangle=90)
plt.title("用户行为类型分布")

# 行为类型柱状图
plt.subplot(1, 2, 2)
sns.barplot(x=behavior_counts.index, y=behavior_counts.values)
plt.title("用户行为数量统计")
plt.ylabel("行为次数")
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

# 时间特征提取
print("\n=== 时间特征工程 ===")
# 时间戳转换为日期时间
df["datetime"] = pd.to_datetime(df["timestamp"], unit="s")
df["date"] = df["datetime"].dt.date
df["hour"] = df["datetime"].dt.hour  # 小时（0-23）
df["day_of_week"] = df["datetime"].dt.dayofweek  # 星期几（0=周一，6=周日）
df["is_weekend"] = (df["day_of_week"] >= 5).astype(int)  # 是否周末

print(f"数据时间范围：{df['datetime'].min()} 至 {df['datetime'].max()}")

# 分析用户行为的时间模式
hourly_behavior = df.groupby(['hour', 'behavior']).size().unstack(fill_value=0)
daily_behavior = df.groupby(['day_of_week', 'behavior']).size().unstack(fill_value=0)

# 可视化时间模式
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# 1. 每小时行为分布
hourly_behavior.plot(kind='line', ax=axes[0, 0], marker='o')
axes[0, 0].set_title("24小时用户行为分布")
axes[0, 0].set_xlabel("小时")
axes[0, 0].set_ylabel("行为次数")
axes[0, 0].legend(title="行为类型")
axes[0, 0].grid(True, alpha=0.3)

# 2. 每周行为分布
day_labels = ['周一', '周二', '周三', '周四', '周五', '周六', '周日']
daily_behavior.plot(kind='bar', ax=axes[0, 1])
axes[0, 1].set_title("一周用户行为分布")
axes[0, 1].set_xlabel("星期")
axes[0, 1].set_ylabel("行为次数")
axes[0, 1].set_xticklabels(day_labels, rotation=45)
axes[0, 1].legend(title="行为类型")

# 3. 购买行为的时间热力图
buy_hourly = df[df['behavior'] == 'buy'].groupby(['day_of_week', 'hour']).size().unstack(fill_value=0)
sns.heatmap(buy_hourly, annot=False, cmap='YlOrRd', ax=axes[1, 0])
axes[1, 0].set_title("购买行为时间热力图")
axes[1, 0].set_xlabel("小时")
axes[1, 0].set_ylabel("星期")
axes[1, 0].set_yticklabels(day_labels, rotation=0)

# 4. 转化漏斗分析
funnel_data = df.groupby('user_id')['behavior'].apply(lambda x: x.value_counts()).unstack(fill_value=0)
funnel_stats = {
    '浏览用户': (funnel_data['pv'] > 0).sum(),
    '收藏用户': (funnel_data['fav'] > 0).sum(),
    '加购用户': (funnel_data['cart'] > 0).sum(),
    '购买用户': (funnel_data['buy'] > 0).sum()
}

funnel_values = list(funnel_stats.values())
funnel_labels = list(funnel_stats.keys())
conversion_rates = [funnel_values[i]/funnel_values[0]*100 for i in range(len(funnel_values))]

bars = axes[1, 1].bar(funnel_labels, funnel_values, color=['skyblue', 'lightgreen', 'orange', 'red'])
axes[1, 1].set_title("用户行为转化漏斗")
axes[1, 1].set_ylabel("用户数")

# 在柱状图上添加转化率标签
for i, (bar, rate) in enumerate(zip(bars, conversion_rates)):
    height = bar.get_height()
    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                    f'{rate:.1f}%', ha='center', va='bottom')

plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# 用户行为特征工程
print("\n=== 用户行为特征工程 ===")

# 1. 用户活跃度特征
user_activity = df.groupby('user_id').agg({
    'behavior': 'count',  # 总行为次数
    'item_id': 'nunique',  # 浏览商品数
    'category_id': 'nunique',  # 浏览类别数
    'datetime': ['min', 'max']  # 首次和最后活跃时间
}).round(2)

user_activity.columns = ['total_behaviors', 'unique_items', 'unique_categories', 'first_active', 'last_active']

# 计算活跃天数
user_activity['active_days'] = (user_activity['last_active'] - user_activity['first_active']).dt.days + 1
user_activity['behaviors_per_day'] = user_activity['total_behaviors'] / user_activity['active_days']

# 2. 用户行为偏好特征
user_behavior_pivot = df.groupby(['user_id', 'behavior']).size().unstack(fill_value=0)
user_behavior_pivot.columns = [f'{col}_count' for col in user_behavior_pivot.columns]

# 计算行为比例
user_behavior_pivot['total_actions'] = user_behavior_pivot.sum(axis=1)
for col in ['pv_count', 'cart_count', 'fav_count', 'buy_count']:
    if col in user_behavior_pivot.columns:
        user_behavior_pivot[f'{col}_ratio'] = user_behavior_pivot[col] / user_behavior_pivot['total_actions']

# 3. 用户时间偏好特征
user_time_features = df.groupby('user_id').agg({
    'hour': ['mean', 'std'],  # 平均活跃时间和时间分散度
    'is_weekend': 'mean'  # 周末活跃比例
}).round(2)

user_time_features.columns = ['avg_active_hour', 'hour_std', 'weekend_ratio']

# 4. 构建目标变量：用户是否有购买行为
user_targets = df.groupby('user_id')['behavior'].apply(lambda x: (x == 'buy').any()).astype(int)
user_targets.name = 'has_purchase'

# 合并所有特征
print("合并用户特征...")
user_features = user_activity.join([user_behavior_pivot, user_time_features, user_targets], how='inner')

# 填充缺失值
user_features = user_features.fillna(0)

print(f"特征工程完成！")
print(f"- 用户数：{len(user_features):,}")
print(f"- 特征数：{len(user_features.columns)-1}")  # 减去目标变量
print(f"- 购买用户比例：{user_features['has_purchase'].mean():.2%}")

# 显示特征统计信息
print(f"\n主要特征统计：")
key_features = ['total_behaviors', 'unique_items', 'unique_categories', 'behaviors_per_day', 
                'pv_count', 'cart_count', 'fav_count', 'buy_count']
print(user_features[key_features].describe())

# 准备机器学习数据
print(f"\n=== 准备机器学习数据 ===")
# 选择特征（排除时间相关和目标变量）
feature_columns = [col for col in user_features.columns 
                  if col not in ['has_purchase', 'first_active', 'last_active']]

X = user_features[feature_columns]
y = user_features['has_purchase']

# 数据集划分
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"训练集：{X_train.shape[0]:,} 样本，{X_train.shape[1]} 特征")
print(f"测试集：{X_test.shape[0]:,} 样本")
print(f"训练集购买率：{y_train.mean():.2%}")
print(f"测试集购买率：{y_test.mean():.2%}")

# 特征重要性预分析（基于相关性）
print(f"\n特征与目标变量的相关性（Top 10）：")
correlations = X.corrwith(y).abs().sort_values(ascending=False)
print(correlations.head(10))
```


### 5.3 决策树模型构建与优化
```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import seaborn as sns

print("=== 决策树模型训练与评估 ===")

# 1. 基础决策树模型
print("训练基础决策树模型...")
clf_basic = DecisionTreeClassifier(random_state=42)
clf_basic.fit(X_train, y_train)

# 基础模型预测
y_pred_basic = clf_basic.predict(X_test)
y_prob_basic = clf_basic.predict_proba(X_test)[:, 1]

# 2. 优化决策树模型（应用剪枝技术）
print("训练优化决策树模型（预剪枝+后剪枝）...")
clf_optimized = DecisionTreeClassifier(
    max_depth=8,           # 预剪枝：限制最大深度
    min_samples_split=50,  # 预剪枝：最小分裂样本数
    min_samples_leaf=20,   # 预剪枝：最小叶节点样本数
    ccp_alpha=0.001,       # 后剪枝：成本复杂度参数
    random_state=42
)
clf_optimized.fit(X_train, y_train)

# 优化模型预测
y_pred_optimized = clf_optimized.predict(X_test)
y_prob_optimized = clf_optimized.predict_proba(X_test)[:, 1]

# 3. 模型性能对比
def evaluate_model(y_true, y_pred, y_prob, model_name):
    """评估模型性能的函数"""
    acc = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    auc = roc_auc_score(y_true, y_prob)
    
    print(f"\n{model_name} 性能指标：")
    print(f"- 准确率 (Accuracy): {acc:.4f}")
    print(f"- 精确率 (Precision): {precision:.4f}")
    print(f"- 召回率 (Recall): {recall:.4f}")
    print(f"- F1分数: {f1:.4f}")
    print(f"- AUC值: {auc:.4f}")
    
    return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1, 'auc': auc}

# 评估两个模型
basic_metrics = evaluate_model(y_test, y_pred_basic, y_prob_basic, "基础决策树")
optimized_metrics = evaluate_model(y_test, y_pred_optimized, y_prob_optimized, "优化决策树")

# 4. 可视化模型对比
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# 4.1 混淆矩阵对比
cm_basic = confusion_matrix(y_test, y_pred_basic)
cm_optimized = confusion_matrix(y_test, y_pred_optimized)

sns.heatmap(cm_basic, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0])
axes[0, 0].set_title("基础决策树 - 混淆矩阵")
axes[0, 0].set_xlabel("预测标签")
axes[0, 0].set_ylabel("真实标签")

sns.heatmap(cm_optimized, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1])
axes[0, 1].set_title("优化决策树 - 混淆矩阵")
axes[0, 1].set_xlabel("预测标签")
axes[0, 1].set_ylabel("真实标签")

# 4.2 ROC曲线对比
fpr_basic, tpr_basic, _ = roc_curve(y_test, y_prob_basic)
fpr_opt, tpr_opt, _ = roc_curve(y_test, y_prob_optimized)

axes[0, 2].plot(fpr_basic, tpr_basic, label=f'基础决策树 (AUC={basic_metrics["auc"]:.3f})')
axes[0, 2].plot(fpr_opt, tpr_opt, label=f'优化决策树 (AUC={optimized_metrics["auc"]:.3f})')
axes[0, 2].plot([0, 1], [0, 1], 'k--', alpha=0.5)
axes[0, 2].set_xlabel("假正率 (FPR)")
axes[0, 2].set_ylabel("真正率 (TPR)")
axes[0, 2].set_title("ROC曲线对比")
axes[0, 2].legend()
axes[0, 2].grid(True, alpha=0.3)

# 4.3 性能指标对比
metrics_comparison = pd.DataFrame([basic_metrics, optimized_metrics], 
                                 index=['基础决策树', '优化决策树'])
metrics_comparison.plot(kind='bar', ax=axes[1, 0])
axes[1, 0].set_title("模型性能指标对比")
axes[1, 0].set_ylabel("分数")
axes[1, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')
axes[1, 0].tick_params(axis='x', rotation=45)

# 4.4 树复杂度对比
tree_stats = pd.DataFrame({
    '基础决策树': [clf_basic.get_depth(), clf_basic.get_n_leaves()],
    '优化决策树': [clf_optimized.get_depth(), clf_optimized.get_n_leaves()]
}, index=['树深度', '叶节点数'])

tree_stats.plot(kind='bar', ax=axes[1, 1])
axes[1, 1].set_title("树复杂度对比")
axes[1, 1].set_ylabel("数量")
axes[1, 1].tick_params(axis='x', rotation=45)

# 4.5 特征重要性分析（使用优化模型）
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': clf_optimized.feature_importances_
}).sort_values('importance', ascending=False).head(15)

sns.barplot(data=feature_importance, y='feature', x='importance', ax=axes[1, 2])
axes[1, 2].set_title("Top 15 特征重要性")
axes[1, 2].set_xlabel("重要性分数")

plt.tight_layout()
plt.show()

# 5. 业务洞察分析
print(f"\n=== 业务洞察与特征解读 ===")

# 5.1 最重要特征分析
top_features = feature_importance.head(10)
print(f"Top 10 重要特征：")
for idx, row in top_features.iterrows():
    print(f"- {row['feature']}: {row['importance']:.4f}")

# 5.2 用户行为模式分析
print(f"\n用户购买行为模式分析：")

# 购买用户 vs 非购买用户的特征对比
purchase_users = X[y == 1]
non_purchase_users = X[y == 0]

key_features_analysis = ['total_behaviors', 'pv_count', 'cart_count', 'fav_count', 
                        'unique_items', 'behaviors_per_day', 'avg_active_hour']

comparison_stats = pd.DataFrame({
    '购买用户均值': purchase_users[key_features_analysis].mean(),
    '非购买用户均值': non_purchase_users[key_features_analysis].mean()
})
comparison_stats['差异倍数'] = comparison_stats['购买用户均值'] / comparison_stats['非购买用户均值']

print(comparison_stats.round(2))

# 5.3 决策规则提取（简化版）
print(f"\n=== 关键决策规则提取 ===")
print(f"基于决策树模型，我们可以提取以下购买预测规则：")

# 分析高购买概率的用户特征
high_prob_mask = y_prob_optimized > 0.7
if high_prob_mask.sum() > 0:
    high_prob_features = X_test[high_prob_mask][key_features_analysis].mean()
    print(f"\n高购买概率用户特征（预测概率>0.7）：")
    for feature, value in high_prob_features.items():
        print(f"- {feature}: {value:.2f}")

# 6. 模型部署建议
print(f"\n=== 模型部署与应用建议 ===")
print(f"1. 模型性能：优化后的决策树在测试集上达到 {optimized_metrics['accuracy']:.1%} 的准确率")
print(f"2. 业务价值：能够识别 {optimized_metrics['recall']:.1%} 的潜在购买用户")
print(f"3. 精准度：预测为购买的用户中，{optimized_metrics['precision']:.1%} 确实会购买")
print(f"4. 应用场景：")
print(f"   - 营销活动：向高购买概率用户推送优惠券")
print(f"   - 个性化推荐：调整推荐算法权重")
print(f"   - 库存管理：基于购买预测优化库存")
print(f"5. 模型监控：建议定期重训练，监控特征重要性变化")
```

### 5.4 模型解释与业务应用
**关键发现**：
1. **用户活跃度是最强预测因子**：总行为次数、浏览商品数等活跃度指标对购买行为影响最大
2. **购物车和收藏行为是强信号**：加购物车和收藏行为显著提升购买概率
3. **时间模式有预测价值**：用户的活跃时间模式能够反映购买倾向
4. **行为多样性重要**：浏览不同类别商品的用户更可能购买

**业务应用策略**：
- **精准营销**：对高活跃度+有加购/收藏行为的用户进行重点营销
- **个性化推荐**：基于用户的类别偏好和活跃时间优化推荐时机
- **用户分层**：根据决策树规则将用户分为不同购买概率层级
- **实时预测**：部署模型进行实时购买概率评分，支持动态营销决策


## 6. 思考题
1. **理论题**：对比信息熵与基尼系数在特征选择时的差异，为什么基尼系数计算速度通常更快？  
2. **实践题**：在回归实验中，若 `min_samples_leaf` 从1增加到20，模型的过拟合程度会如何变化？MSE和R²会如何变化？  
3. **拓展题**：决策树对高维稀疏数据（如文本特征）容易过拟合，结合剪枝技术和特征选择方法，提出两种改进策略。  


## 7. 代码填空题答案
- ① `'entropy'`（或 `'gini'`，信息增益或基尼系数）  
- ② `3`（最大深度设为3可避免过拟合）  
- ③ `5`（叶节点最小样本数设为5，增强泛化能力）