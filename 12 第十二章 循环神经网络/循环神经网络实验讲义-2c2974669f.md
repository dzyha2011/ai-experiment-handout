# 循环神经网络实验讲义

## 一、循环神经网络基本概念

### 1.1 序列数据与循环神经网络的必要性
在传统的前馈神经网络中，输入数据之间是相互独立的，无法处理具有时间序列特性的数据（如文本、语音、股票价格等）。**循环神经网络（Recurrent Neural Networks, RNN）** 通过引入**时间维度的记忆功能**，能够处理任意长度的序列数据。其核心特点是：网络在时刻$t$的输出不仅依赖于当前输入，还依赖于之前的隐藏状态（即“记忆”）。

### 1.2 RNN的网络结构与数学原理
#### 1.2.1 基本结构
RNN的基本单元（如**Elman网络**）由输入层、隐藏层和输出层组成，隐藏层在不同时间步之间共享参数。结构如下：
- **输入**：$x_t$（时刻$t$的输入向量）
- **隐藏状态**：$h_t = \tanh(W_{xh}x_t + W_{hh}h_{t-1} + b_h)$  
  其中$W_{xh}$为输入到隐藏层的权重，$W_{hh}$为隐藏层到隐藏层的权重（体现记忆功能），$b_h$为偏置，$\tanh$为激活函数。
- **输出**：$y_t = W_{hy}h_t + b_y$（分类任务中可添加Softmax激活）

#### 1.2.2 梯度消失与梯度爆炸问题
RNN在训练过程中容易出现**梯度消失/爆炸**（Vanishing/Exploding Gradients）。当时间步较长时，反向传播过程中梯度通过$W_{hh}$的连乘可能急剧衰减或放大，导致模型难以学习长期依赖关系。例如，若$W_{hh}$的特征值小于1，多次连乘后梯度会趋近于0（消失）；若大于1，则梯度会指数增长（爆炸）。

### 1.3 RNN的变体
为解决长期依赖问题，研究者提出了多种RNN变体：
- **GRU（Gated Recurrent Unit）**：简化版LSTM，合并遗忘门和输入门为更新门，减少参数数量。
- **双向RNN（Bidirectional RNN）**：同时利用过去和未来的序列信息，适用于文本翻译等任务。
- **深层RNN（Deep RNN）**：堆叠多个隐藏层，增强特征提取能力。


## 二、长短时记忆网络（LSTM）原理

### 2.1 LSTM的核心改进
**长短时记忆网络（Long Short-Term Memory, LSTM）** 通过设计**门控机制**（Gates）和**细胞状态**（Cell State），有效解决了RNN的梯度消失问题，能够学习长期依赖关系。其核心结构包括：
- **细胞状态（$C_t$）**：类似传送带，信息在其中流动时仅通过少量线性交互修改，保持长期记忆。
- **门控单元**：通过Sigmoid和$\tanh$函数控制信息的流入、流出和遗忘。

### 2.2 LSTM的门控机制详解
#### 2.2.1 遗忘门（Forget Gate）
- **功能**：决定从细胞状态中丢弃哪些信息。
- **公式**：$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$  
  其中$\sigma$为Sigmoid函数（输出0~1，1表示完全保留，0表示完全遗忘），$[h_{t-1}, x_t]$为拼接的上一时刻隐藏状态和当前输入。

#### 2.2.2 输入门（Input Gate）
- **功能**：决定哪些新信息被存放在细胞状态中。
- **步骤**：
  1. **候选值**：$\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$（生成新的候选记忆）。
  2. **更新权重**：$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$（控制候选值的保留比例）。
  3. **细胞状态更新**：$C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t$（$\odot$表示逐元素相乘）。

#### 2.2.3 输出门（Output Gate）
- **功能**：决定当前隐藏状态$h_t$的值（用于输出或传递给下一时间步）。
- **公式**：
  1. $o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$（控制输出比例）。
  2. $h_t = o_t \odot \tanh(C_t)$（隐藏状态为细胞状态的非线性变换与输出门的乘积）。

### 2.3 LSTM与传统RNN的对比
| **特性**         | **传统RNN**                | **LSTM**                      |
|------------------|---------------------------|-------------------------------|
| **长期依赖**     | 难以学习（梯度消失）       | 通过细胞状态和门控机制有效解决 |
| **参数数量**     | 较少                      | 较多（门控单元增加参数）       |
| **适用场景**     | 短序列任务（如语音片段）   | 长序列任务（如文本生成、股价预测） |


## 三、Python实现循环神经网络

### 3.1 实验目标
使用**Keras内置的IMDB影评数据集**（文本序列数据），构建RNN模型实现情感分类（正面/负面影评）。

### 3.2 数据集介绍
IMDB数据集包含50,000条电影评论，分为训练集（25,000）和测试集（25,000），每条评论已被转换为整数序列（单词索引），标签为0（负面）或1（正面）。

### 3.3 代码实现（含填空）

#### 3.3.1 环境准备
```python
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torch.nn.utils.rnn import pad_sequence
from sklearn.model_selection import train_test_split
import requests
import pickle
import os

# 设置设备
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"使用设备: {device}")

# 下载IMDB数据集（使用PyTorch兼容格式）
def download_imdb_data():
    """下载并加载IMDB数据集"""
    url = "https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
    # 这里我们使用一个简化的数据加载方式
    # 实际使用中可以从torchtext或其他源加载
    print("正在加载IMDB数据集...")
    
    # 模拟数据加载（实际项目中应使用真实数据）
    np.random.seed(42)
    vocab_size = 10000
    max_len = 500
    num_samples = 25000
    
    # 生成模拟的序列数据
    x_train = [np.random.randint(1, vocab_size, size=np.random.randint(50, max_len)) 
               for _ in range(num_samples)]
    y_train = np.random.randint(0, 2, size=num_samples)
    
    x_test = [np.random.randint(1, vocab_size, size=np.random.randint(50, max_len)) 
              for _ in range(num_samples)]
    y_test = np.random.randint(0, 2, size=num_samples)
    
    return (x_train, y_train), (x_test, y_test)

# 数据预处理函数
def pad_sequences_pytorch(sequences, maxlen=None, padding='pre', truncating='pre', value=0):
    """PyTorch版本的序列填充函数"""
    if maxlen is None:
        maxlen = max(len(seq) for seq in sequences)
    
    padded_sequences = []
    for seq in sequences:
        seq = list(seq)
        if len(seq) > maxlen:
            if truncating == 'pre':
                seq = seq[-maxlen:]
            else:
                seq = seq[:maxlen]
        
        if len(seq) < maxlen:
            if padding == 'pre':
                seq = [value] * (maxlen - len(seq)) + seq
            else:
                seq = seq + [value] * (maxlen - len(seq))
        
        padded_sequences.append(seq)
    
    return np.array(padded_sequences)

# 加载数据集（保留前10000个最常见的单词）
max_features = 10000
maxlen = 500  # 每条评论截断/填充至500个单词
batch_size = 32

(x_train, y_train), (x_test, y_test) = download_imdb_data()
print(f"训练集样本数：{len(x_train)}, 测试集样本数：{len(x_test)}")

# 数据预处理：统一序列长度
x_train = pad_sequences_pytorch(x_train, maxlen=maxlen)
x_test = pad_sequences_pytorch(x_test, maxlen=maxlen)
print(f"处理后训练集形状：{x_train.shape}, 测试集形状：{x_test.shape}")

# 转换为PyTorch张量
x_train = torch.LongTensor(x_train)
y_train = torch.FloatTensor(y_train)
x_test = torch.LongTensor(x_test)
y_test = torch.FloatTensor(y_test)

# 创建数据加载器
train_dataset = TensorDataset(x_train, y_train)
test_dataset = TensorDataset(x_test, y_test)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
```

#### 3.3.2 模型构建与训练（填空部分）
```python
# 定义RNN模型类
class SimpleRNNModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):
        super(SimpleRNNModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        # 嵌入层
        embedded = self.embedding(x)  # [batch_size, seq_len, embedding_dim]
        
        # RNN层
        rnn_out, hidden = self.rnn(embedded)  # rnn_out: [batch_size, seq_len, hidden_dim]
        
        # 取最后一个时间步的输出
        last_output = rnn_out[:, -1, :]  # [batch_size, hidden_dim]
        
        # 全连接层和激活函数
        output = self.fc(last_output)  # [batch_size, output_dim]
        output = self.sigmoid(output)
        
        return output

# 构建RNN模型
embedding_dim = 32
hidden_dim = ____  # 填空1：隐藏单元数量，建议填32
output_dim = ____  # 填空2：输出维度，二分类任务填1

model = SimpleRNNModel(max_features, embedding_dim, hidden_dim, output_dim)
model = model.to(device)

# 定义损失函数和优化器
criterion = nn.____()  # 填空3：损失函数，二分类任务使用BCELoss
optimizer = optim.____()  # 填空4：优化器，建议使用RMSprop或Adam

print(f"模型参数数量: {sum(p.numel() for p in model.parameters() if p.requires_grad)}")

# 训练函数
def train_model(model, train_loader, criterion, optimizer, num_epochs):
    model.train()
    train_losses = []
    train_accuracies = []
    
    for epoch in range(num_epochs):
        running_loss = 0.0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            
            # 前向传播
            optimizer.zero_grad()
            output = model(data).squeeze()
            loss = criterion(output, target)
            
            # 反向传播
            loss.backward()
            optimizer.step()
            
            # 统计
            running_loss += loss.item()
            predicted = (output > 0.5).float()
            total += target.size(0)
            correct += (predicted == target).sum().item()
            
            if batch_idx % 100 == 0:
                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}, Loss: {loss.item():.4f}')
        
        epoch_loss = running_loss / len(train_loader)
        epoch_acc = correct / total
        train_losses.append(epoch_loss)
        train_accuracies.append(epoch_acc)
        
        print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')
    
    return train_losses, train_accuracies

# 训练模型
num_epochs = ____  # 填空5：训练轮数，建议填10
train_losses, train_accuracies = train_model(model, train_loader, criterion, optimizer, num_epochs)
```

#### 3.3.3 模型评估与可视化
```python
# 评估函数
def evaluate_model(model, test_loader, criterion):
    model.eval()
    test_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data).squeeze()
            test_loss += criterion(output, target).item()
            
            predicted = (output > 0.5).float()
            total += target.size(0)
            correct += (predicted == target).sum().item()
    
    test_loss /= len(test_loader)
    test_acc = correct / total
    
    return test_loss, test_acc

# 在测试集上评估
test_loss, test_acc = evaluate_model(model, test_loader, criterion)
print(f"测试集准确率：{test_acc:.4f}")
print(f"测试集损失：{test_loss:.4f}")

# 绘制训练过程中的准确率和损失曲线
epochs = range(1, len(train_losses) + 1)

plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(epochs, train_accuracies, 'bo-', label='Training accuracy')
plt.title('Training Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(epochs, train_losses, 'ro-', label='Training loss')
plt.title('Training Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

# 保存模型
torch.save(model.state_dict(), 'simple_rnn_model.pth')
print("模型已保存为 'simple_rnn_model.pth'")
```

### 代码填空答案
1. 隐藏单元数量：`32`  
2. 输出维度：`1`  
3. 损失函数：`BCELoss`  
4. 优化器：`RMSprop(model.parameters(), lr=0.001)` 或 `Adam(model.parameters(), lr=0.001)`  
5. 训练轮数：`10`

## 附录：完整可运行代码示例

### A.1 IMDB情感分类完整代码
```python
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# 设置随机种子以确保结果可重现
torch.manual_seed(42)
np.random.seed(42)

# 设置设备
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"使用设备: {device}")

# 模拟IMDB数据集
def create_imdb_data():
    vocab_size = 10000
    max_len = 500
    num_samples = 1000  # 为了快速演示，使用较小的数据集
    
    x_train = [np.random.randint(1, vocab_size, size=np.random.randint(50, max_len)) 
               for _ in range(num_samples)]
    y_train = np.random.randint(0, 2, size=num_samples)
    
    x_test = [np.random.randint(1, vocab_size, size=np.random.randint(50, max_len)) 
              for _ in range(num_samples//4)]
    y_test = np.random.randint(0, 2, size=num_samples//4)
    
    return (x_train, y_train), (x_test, y_test)

# 数据预处理
def pad_sequences_pytorch(sequences, maxlen=500, value=0):
    padded_sequences = []
    for seq in sequences:
        seq = list(seq)
        if len(seq) > maxlen:
            seq = seq[-maxlen:]
        if len(seq) < maxlen:
            seq = [value] * (maxlen - len(seq)) + seq
        padded_sequences.append(seq)
    return np.array(padded_sequences)

# RNN模型定义
class SimpleRNNModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):
        super(SimpleRNNModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        embedded = self.embedding(x)
        rnn_out, hidden = self.rnn(embedded)
        last_output = rnn_out[:, -1, :]
        output = self.fc(last_output)
        output = self.sigmoid(output)
        return output

# 主程序
if __name__ == "__main__":
    # 加载数据
    (x_train, y_train), (x_test, y_test) = create_imdb_data()
    
    # 预处理
    max_features = 10000
    maxlen = 500
    x_train = pad_sequences_pytorch(x_train, maxlen=maxlen)
    x_test = pad_sequences_pytorch(x_test, maxlen=maxlen)
    
    # 转换为张量
    x_train = torch.LongTensor(x_train)
    y_train = torch.FloatTensor(y_train)
    x_test = torch.LongTensor(x_test)
    y_test = torch.FloatTensor(y_test)
    
    # 创建数据加载器
    train_dataset = TensorDataset(x_train, y_train)
    test_dataset = TensorDataset(x_test, y_test)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    # 构建模型
    model = SimpleRNNModel(max_features, 32, 32, 1)
    model = model.to(device)
    
    # 定义损失函数和优化器
    criterion = nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # 训练模型
    num_epochs = 10
    train_losses = []
    train_accuracies = []
    
    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        for data, target in train_loader:
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            output = model(data).squeeze()
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            predicted = (output > 0.5).float()
            total += target.size(0)
            correct += (predicted == target).sum().item()
        
        epoch_loss = running_loss / len(train_loader)
        epoch_acc = correct / total
        train_losses.append(epoch_loss)
        train_accuracies.append(epoch_acc)
        
        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')
    
    # 测试模型
    model.eval()
    test_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data).squeeze()
            test_loss += criterion(output, target).item()
            
            predicted = (output > 0.5).float()
            total += target.size(0)
            correct += (predicted == target).sum().item()
    
    test_acc = correct / total
    print(f"测试集准确率：{test_acc:.4f}")
    
    # 可视化训练过程
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(range(1, num_epochs + 1), train_accuracies, 'bo-')
    plt.title('Training Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    
    plt.subplot(1, 2, 2)
    plt.plot(range(1, num_epochs + 1), train_losses, 'ro-')
    plt.title('Training Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    
    plt.tight_layout()
    plt.show()
```

### A.2 股票价格预测完整代码
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import MinMaxScaler

# 设置随机种子
torch.manual_seed(42)
np.random.seed(42)

# 生成模拟股票数据
def generate_stock_data(n_days=1000):
    """生成模拟股票价格数据"""
    dates = pd.date_range(start='2020-01-01', periods=n_days, freq='D')
    
    # 生成带趋势的随机游走
    price = 100  # 初始价格
    prices = [price]
    
    for i in range(1, n_days):
        # 添加趋势和随机波动
        trend = 0.0005  # 轻微上升趋势
        volatility = 0.02
        change = np.random.normal(trend, volatility)
        price = price * (1 + change)
        prices.append(price)
    
    df = pd.DataFrame({
        'Date': dates,
        'Close': prices
    })
    df.set_index('Date', inplace=True)
    
    return df

# LSTM模型定义
class StockLSTM(nn.Module):
    def __init__(self, input_size=1, hidden_size=50, num_layers=2, output_size=1):
        super(StockLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc1 = nn.Linear(hidden_size, 25)
        self.fc2 = nn.Linear(25, output_size)
        self.relu = nn.ReLU()
        
    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        
        lstm_out, _ = self.lstm(x, (h0, c0))
        last_output = lstm_out[:, -1, :]
        
        out = self.relu(self.fc1(last_output))
        out = self.fc2(out)
        
        return out

# 创建序列数据
def create_sequences(data, seq_length):
    X, y = [], []
    for i in range(seq_length, len(data)):
        X.append(data[i-seq_length:i, 0])
        y.append(data[i, 0])
    return np.array(X), np.array(y)

# 主程序
if __name__ == "__main__":
    # 生成数据
    df = generate_stock_data(1000)
    print(f"数据形状: {df.shape}")
    
    # 数据预处理
    scaler = MinMaxScaler(feature_range=(0, 1))
    df_scaled = scaler.fit_transform(df[['Close']])
    
    # 创建序列
    seq_length = 60
    X, y = create_sequences(df_scaled, seq_length)
    X = X.reshape(X.shape[0], X.shape[1], 1)
    
    # 划分训练集和测试集
    train_size = int(0.8 * len(X))
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]
    
    # 转换为张量
    X_train = torch.FloatTensor(X_train)
    y_train = torch.FloatTensor(y_train).view(-1, 1)
    X_test = torch.FloatTensor(X_test)
    y_test = torch.FloatTensor(y_test).view(-1, 1)
    
    # 创建数据加载器
    train_dataset = TensorDataset(X_train, y_train)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    
    # 设置设备
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # 构建模型
    model = StockLSTM(input_size=1, hidden_size=50, num_layers=2, output_size=1)
    model = model.to(device)
    
    # 定义损失函数和优化器
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # 训练模型
    num_epochs = 20
    model.train()
    
    for epoch in range(num_epochs):
        running_loss = 0.0
        for data, target in train_loader:
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
        
        avg_loss = running_loss / len(train_loader)
        if (epoch + 1) % 5 == 0:
            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.6f}')
    
    # 预测
    model.eval()
    with torch.no_grad():
        X_test = X_test.to(device)
        predictions = model(X_test).cpu().numpy()
    
    # 反归一化
    predictions = scaler.inverse_transform(predictions)
    y_test_original = scaler.inverse_transform(y_test.numpy())
    
    # 计算评估指标
    rmse = np.sqrt(np.mean((predictions - y_test_original) ** 2))
    mae = np.mean(np.abs(predictions - y_test_original))
    
    print(f"测试集RMSE：{rmse:.4f}")
    print(f"测试集MAE：{mae:.4f}")
    
    # 可视化结果
    plt.figure(figsize=(15, 6))
    plt.plot(y_test_original, label='Actual Price', linewidth=2)
    plt.plot(predictions, label='Predicted Price', linewidth=2)
    plt.title('Stock Price Prediction')
    plt.xlabel('Time')
    plt.ylabel('Price')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()
```  


## 四、序列数据处理与实验应用

### 4.1 序列数据的特点与挑战
- **变长性**：不同样本的序列长度可能不同（如句子长度、时间序列长度）。
- **时间依赖性**：数据点之间存在时间顺序关系，不可随意打乱。
- **噪声干扰**：实际序列数据（如传感器数据、股价）常包含噪声，需预处理。

### 4.2 序列数据预处理关键步骤
#### 4.2.1 标准化/归一化
- **目的**：消除量纲影响，加速模型收敛。  
- **方法**：对时间序列数据常用**Min-Max归一化**（将值缩放到[0,1]）：  
  $x' = \frac{x - \min(x)}{\max(x) - \min(x)}$

#### 4.2.2 序列截断与填充
- **截断（Truncation）**：对过长序列截取固定长度（如前500个时间步）。  
- **填充（Padding）**：对过短序列在开头或结尾补0（如使用`pad_sequences`）。  

#### 4.2.3 时间步划分
- 将长序列转换为多个输入-输出样本对。例如，用前$t$个时刻的数据预测第$t+1$个时刻的值：  
  输入：$[x_1, x_2, ..., x_t]$，输出：$x_{t+1}$

### 4.3 实验应用：IMDB情感分类结果分析
- **模型性能**：SimpleRNN在IMDB测试集上的准确率约为85%（受超参数影响）。  
- **过拟合问题**：训练后期验证集准确率下降，可通过**Dropout**或**早停（Early Stopping）** 缓解。  
- **改进方向**：替换为LSTM/GRU可提升性能（捕捉长期依赖），如LSTM模型准确率可达88%~90%。


## 五、实例拓展-股票价格走势预测

### 5.1 问题背景
股票价格是典型的时间序列数据，受历史价格、交易量、市场情绪等多因素影响。LSTM模型通过学习历史价格序列的模式，可预测未来短期价格走势。

### 5.2 数据集介绍与获取
**推荐数据集**：Kaggle上的“Complete Microsoft Stock Dataset (1986–2025)”  
- **链接**：https://www.kaggle.com/datasets/muhammadatiflatif/complete-microsoft-stock-dataset-19862025  
- **内容**：包含Microsoft（MSFT）从1986年3月到2025年4月的日度数据，共9,843条记录，特征包括：  
  `Date, Open, High, Low, Close, Adj Close, Volume`  
- **格式**：CSV文件，可直接用Pandas读取。

### 5.3 数据预处理步骤
1. **加载数据**：
   ```python
   import pandas as pd
   import numpy as np
   from sklearn.preprocessing import MinMaxScaler
   import torch
   
   df = pd.read_csv('MSFT_1986-03-13_2025-04-06.csv', parse_dates=['Date'], index_col='Date')
   ```
2. **选择特征**：以`Close`（收盘价）作为预测目标，可加入`Volume`等辅助特征。
3. **归一化**：
   ```python
   scaler = MinMaxScaler(feature_range=(0, 1))
   df_scaled = scaler.fit_transform(df[['Close']])
   ```
4. **构建序列样本**：
   ```python
   def create_sequences(data, seq_length):
       """创建时间序列样本"""
       X, y = [], []
       for i in range(seq_length, len(data)):
           X.append(data[i-seq_length:i, 0])  # 前seq_length天的价格
           y.append(data[i, 0])  # 第i天的价格
       return np.array(X), np.array(y)
   
   seq_length = 60  # 使用过去60天数据预测未来1天
   X, y = create_sequences(df_scaled, seq_length)
   X = X.reshape(X.shape[0], X.shape[1], 1)  # 转换为LSTM输入格式：[样本数, 时间步, 特征数]
   
   print(f"输入数据形状: {X.shape}")
   print(f"输出数据形状: {y.shape}")
   ```

### 5.4 LSTM模型构建与预测
```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# 定义LSTM模型类
class StockLSTM(nn.Module):
    def __init__(self, input_size=1, hidden_size=50, num_layers=2, output_size=1):
        super(StockLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # LSTM层
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        
        # 全连接层
        self.fc1 = nn.Linear(hidden_size, 25)
        self.fc2 = nn.Linear(25, output_size)
        self.relu = nn.ReLU()
        
    def forward(self, x):
        # 初始化隐藏状态和细胞状态
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        
        # LSTM前向传播
        lstm_out, _ = self.lstm(x, (h0, c0))
        
        # 取最后一个时间步的输出
        last_output = lstm_out[:, -1, :]
        
        # 全连接层
        out = self.relu(self.fc1(last_output))
        out = self.fc2(out)
        
        return out

# 划分训练集和测试集（8:2）
train_size = int(0.8 * len(X))
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# 转换为PyTorch张量
X_train = torch.FloatTensor(X_train)
y_train = torch.FloatTensor(y_train).view(-1, 1)
X_test = torch.FloatTensor(X_test)
y_test = torch.FloatTensor(y_test).view(-1, 1)

# 创建数据加载器
train_dataset = TensorDataset(X_train, y_train)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# 设置设备
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# 构建LSTM模型
model = StockLSTM(input_size=1, hidden_size=50, num_layers=2, output_size=1)
model = model.to(device)

# 定义损失函数和优化器
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 训练模型
num_epochs = 20
model.train()

for epoch in range(num_epochs):
    running_loss = 0.0
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        
        # 前向传播
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        
        # 反向传播
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
    
    avg_loss = running_loss / len(train_loader)
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.6f}')

# 预测
model.eval()
with torch.no_grad():
    X_test = X_test.to(device)
    predictions = model(X_test).cpu().numpy()

# 反归一化（将预测值转换为原始价格范围）
predictions = scaler.inverse_transform(predictions)
y_test_original = scaler.inverse_transform(y_test.numpy())
```

### 5.5 结果可视化与评估
```python
import matplotlib.pyplot as plt
import numpy as np

# 绘制真实值与预测值对比曲线
plt.figure(figsize=(16, 8))
plt.title('MSFT Stock Price Prediction')
plt.xlabel('Date', fontsize=18)
plt.ylabel('Close Price USD ($)', fontsize=18)

# 由于我们使用的是模拟数据，这里创建一个时间索引
time_index = range(len(y_test_original))

plt.plot(time_index, y_test_original, 'b', label='Actual Price', linewidth=2)
plt.plot(time_index, predictions, 'r', label='Predicted Price', linewidth=2)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# 评估指标：均方根误差（RMSE）
rmse = np.sqrt(np.mean((predictions - y_test_original) ** 2))
print(f"测试集RMSE：{rmse:.4f}")

# 计算平均绝对误差（MAE）
mae = np.mean(np.abs(predictions - y_test_original))
print(f"测试集MAE：{mae:.4f}")

# 计算平均绝对百分比误差（MAPE）
mape = np.mean(np.abs((y_test_original - predictions) / y_test_original)) * 100
print(f"测试集MAPE：{mape:.2f}%")

# 绘制预测误差分布
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
errors = predictions.flatten() - y_test_original.flatten()
plt.hist(errors, bins=50, alpha=0.7, color='blue', edgecolor='black')
plt.title('Prediction Error Distribution')
plt.xlabel('Error')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.scatter(y_test_original, predictions, alpha=0.6)
plt.plot([y_test_original.min(), y_test_original.max()], 
         [y_test_original.min(), y_test_original.max()], 'r--', lw=2)
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.title('Actual vs Predicted Prices')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# 保存模型
torch.save(model.state_dict(), 'stock_lstm_model.pth')
print("模型已保存为 'stock_lstm_model.pth'")

# 模型加载示例
def load_model_for_prediction():
    """加载已训练的模型进行预测"""
    loaded_model = StockLSTM(input_size=1, hidden_size=50, num_layers=2, output_size=1)
    loaded_model.load_state_dict(torch.load('stock_lstm_model.pth'))
    loaded_model.eval()
    return loaded_model

# 使用示例
# loaded_model = load_model_for_prediction()
```


## 六、思考题

1. **理论题**：  
   对比RNN和LSTM的隐藏层结构，解释LSTM如何通过门控机制解决梯度消失问题。

2. **实验题**：  
   在IMDB实验中，若将`SimpleRNN`替换为`LSTM`，需要修改哪些代码？预期性能会如何变化？为什么？
   
   **答案提示**：
   ```python
   # 将SimpleRNNModel中的RNN层替换为LSTM层
   class LSTMModel(nn.Module):
       def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):
           super(LSTMModel, self).__init__()
           self.embedding = nn.Embedding(vocab_size, embedding_dim)
           self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)  # 替换为LSTM
           self.fc = nn.Linear(hidden_dim, output_dim)
           self.sigmoid = nn.Sigmoid()
           
       def forward(self, x):
           embedded = self.embedding(x)
           lstm_out, (hidden, cell) = self.lstm(embedded)  # LSTM返回hidden和cell状态
           last_output = lstm_out[:, -1, :]
           output = self.fc(last_output)
           output = self.sigmoid(output)
           return output
   ```
   预期LSTM性能会更好（准确率提升2-5%），因为LSTM能更好地捕捉长期依赖关系。

3. **应用题**：  
   在股票预测实验中，若加入技术指标（如RSI、MACD）作为额外特征，数据预处理步骤需要如何调整？这可能对模型性能产生什么影响？

4. **开放题**：  
   列举LSTM在其他领域的应用场景（至少3个），并说明其优势。


## 代码填空答案
1. 隐藏单元数量：`32`  
2. 输出层：`units=1, activation='sigmoid'`  
3. 编译模型：`optimizer='rmsprop', loss='binary_crossentropy'`  
4. 训练轮数：`10`